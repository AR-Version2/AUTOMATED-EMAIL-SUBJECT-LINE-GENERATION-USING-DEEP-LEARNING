# -*- coding: utf-8 -*-
"""6_Analyze_Human_Eval_v3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tm95QI7jN7ZYJWi0fd4MUyijg_Ue1eHU
"""

# -*- coding: utf-8 -*-
"""
6_Analyze_Human_Eval_v4.py

Loads a single CSV containing combined human ratings for multiple raters.
Loads the corresponding keyed evaluation sheet to get subject sources.
Merges the data.
VALIDATES AND CLEANS rating values based on defined scale.
Calculates summary scores for each specified rater INDIVIDUALLY.
Calculates Inter-Annotator Agreement (IAA) between the specified raters.
Saves summary statistics and IAA results.

Uses Tkinter GUI prompts for selecting input files and output directory.
Logs console output to a timestamped file.
"""

import os
import sys
import datetime
import logging
import pandas as pd
import numpy as np
import glob
from typing import List, Dict, Optional, Union, Tuple

# --- Import Tkinter ---
try:
    import tkinter as tk
    from tkinter import filedialog
    tkinter_available = True
except ImportError:
    tkinter_available = False
    print("ERROR: Tkinter library not found.", file=sys.stderr)
    sys.exit(1)

# --- Optional IAA Imports ---
try:
    from sklearn.metrics import cohen_kappa_score
    sklearn_available = True
    print("Scikit-learn found. Cohen's Kappa can be calculated.")
except ImportError:
    sklearn_available = False
    print("WARNING: Scikit-learn not found. Cohen's Kappa cannot be calculated.")

try:
    import krippendorff
    krippendorff_available = True
    print("Krippendorff library found. Krippendorff's Alpha can be calculated.")
except ImportError:
    krippendorff_available = False
    print("WARNING: Krippendorff library not found.")

# --- Configuration Constants ---
# Use Rater 1 and Rater 2
RATERS_TO_USE: List[int] = [1, 2]

RATING_CRITERIA: List[str] = ["Relevance", "Conciseness", "Fluency"]
RATING_SCALE_MIN: int = 1
RATING_SCALE_MAX: int = 4

# --- Dynamically Generate Column Names Based on RATERS_TO_USE ---
RATING_COLS_TO_PROCESS = []
for r in RATERS_TO_USE:
    for crit in RATING_CRITERIA:
        RATING_COLS_TO_PROCESS.append(f"Rater{r}_{crit}")

COMBINED_RATINGS_COLS_BASE = ["SampleID", "EmailBody", "SubjectID", "SubjectToRate"]
COMBINED_RATINGS_COLS_EXPECTED = COMBINED_RATINGS_COLS_BASE + RATING_COLS_TO_PROCESS
KEYED_SHEET_COLS_NEEDED = ["SampleID", "SubjectID", "Source"]

print("\n--- Configuration ---")
print(f"Raters Included in Analysis: {RATERS_TO_USE}")
print(f"Rating Criteria: {RATING_CRITERIA}")
print(f"Rating Scale: {RATING_SCALE_MIN}-{RATING_SCALE_MAX}")
print(f"Expected Rating Columns: {RATING_COLS_TO_PROCESS}")
print("-" * 30)

# --- Logger Class ---
class Logger:
    # (Logger class definition remains the same)
    def __init__(self, filepath, original_stdout, original_stderr):
        self.terminal = original_stdout; self.stderr_orig = original_stderr; self.log_file = None
        try:
            self.log_file = open(filepath, "w", encoding='utf-8', errors='replace')
            print(f"Logging console output to: {filepath}")
        except Exception as e: print(f"FATAL ERROR: Could not open log file {filepath}: {e}", file=self.stderr_orig); self.log_file = None
    def write(self, message):
        try: self.terminal.write(message);
        except Exception as e: print(f"Logger Write Error: {e}", file=self.stderr_orig)
        try:
            if self.log_file: self.log_file.write(message)
        except Exception as e: print(f"Logger File Write Error: {e}", file=self.stderr_orig)
    def flush(self):
        try: self.terminal.flush()
        except Exception as e: print(f"Logger Flush Error (Terminal): {e}", file=self.stderr_orig)
        try:
            if self.log_file: self.log_file.flush()
        except Exception as e: print(f"Logger Flush Error (File): {e}", file=self.stderr_orig)
    def close(self):
        if self.log_file:
            try: self.log_file.close()
            except Exception as e: print(f"Logger Close Error: {e}", file=self.stderr_orig)
            self.log_file = None

# --- Helper Functions ---
def get_timestamp() -> str:
    return datetime.datetime.now().strftime('%Y%m%d_%H%M%S')

def select_folder(title: str) -> Optional[str]:
    # (select_folder remains the same)
    if not tkinter_available: print("ERROR: Tkinter not available."); return None
    print(f"\nA folder selection window should open. Please select the folder for: '{title}'")
    root = tk.Tk(); root.withdraw(); root.attributes('-topmost', True)
    folder_path = filedialog.askdirectory(title=title, mustexist=True)
    root.destroy()
    if folder_path: print(f"  -> Selected: {folder_path}"); return folder_path
    else: print("  -> Folder selection cancelled or failed."); return None

def select_file(title: str, filetypes: List[Tuple[str, str]]) -> Optional[str]:
    # (select_file remains the same)
    if not tkinter_available: print("ERROR: Tkinter not available."); return None
    print(f"\nA file selection window should open. Please select the file for: '{title}'")
    root = tk.Tk(); root.withdraw(); root.attributes('-topmost', True)
    file_path = filedialog.askopenfilename(title=title, filetypes=filetypes)
    root.destroy()
    if file_path: print(f"  -> Selected: {file_path}"); return file_path
    else: print("  -> File selection cancelled or failed."); return None

# --- Core Functions ---

def load_combined_ratings(ratings_file_path: str) -> Optional[pd.DataFrame]:
    """Loads ratings, validates, converts to numeric, checks/replaces out-of-domain."""
    # (Function remains the same as in v3 - it correctly checks columns in RATERS_TO_USE)
    logging.info(f"Loading combined ratings from: {ratings_file_path}")
    if not os.path.exists(ratings_file_path):
        logging.error(f"Ratings file not found: {ratings_file_path}")
        return None
    try:
        df_ratings = pd.read_csv(ratings_file_path, low_memory=False)
        logging.info(f"Loaded {len(df_ratings)} rows from {os.path.basename(ratings_file_path)}. Shape: {df_ratings.shape}")
        missing_cols = [col for col in COMBINED_RATINGS_COLS_EXPECTED if col not in df_ratings.columns]
        if missing_cols:
            logging.error(f"Ratings file {ratings_file_path} is missing required columns: {missing_cols}")
            return None
        logging.info(f"Validated required columns exist: {COMBINED_RATINGS_COLS_EXPECTED}")
        nan_counts_before = df_ratings[RATING_COLS_TO_PROCESS].isnull().sum().sum()
        for col in RATING_COLS_TO_PROCESS:
            df_ratings[col] = pd.to_numeric(df_ratings[col], errors='coerce')
        nan_counts_after = df_ratings[RATING_COLS_TO_PROCESS].isnull().sum().sum()
        converted_nans = nan_counts_after - nan_counts_before
        if converted_nans > 0: logging.warning(f"Converted {converted_nans} non-numeric entries to NaN.")
        logging.info(f"Checking for ratings outside scale [{RATING_SCALE_MIN}-{RATING_SCALE_MAX}]...")
        total_out_of_domain_found = 0
        for col in RATING_COLS_TO_PROCESS:
            out_of_domain_mask = (df_ratings[col].notna() & ((df_ratings[col] < RATING_SCALE_MIN) | (df_ratings[col] > RATING_SCALE_MAX)))
            out_of_domain_indices = df_ratings.index[out_of_domain_mask]
            if not out_of_domain_indices.empty:
                num_found = len(out_of_domain_indices); total_out_of_domain_found += num_found
                logging.warning(f"Found {num_found} out-of-domain values in column '{col}':")
                for idx in out_of_domain_indices[:5]: logging.warning(f"  - Row index {idx}: Value = {df_ratings.loc[idx, col]}")
                if num_found > 5: logging.warning("  - ... (additional values exist)")
                df_ratings.loc[out_of_domain_indices, col] = np.nan
                logging.warning(f"  -> Replaced these {num_found} values in '{col}' with NaN.")
        if total_out_of_domain_found > 0: logging.warning(f"Total out-of-domain values replaced: {total_out_of_domain_found}")
        else: logging.info("No out-of-domain values found.")
        return df_ratings
    except Exception as e:
        logging.error(f"Failed to load/validate ratings file {ratings_file_path}: {e}", exc_info=True)
        return None

# --- >>>>>>>> MODIFICATION: Simplify score calculation (no average rater) <<<<<<<<<< ---
def calculate_scores_per_rater(df_merged_all: pd.DataFrame) -> Tuple[Optional[pd.DataFrame], pd.DataFrame]:
    """Calculates summary statistics (mean, std, count) for each specified rater individually."""
    logging.info(f"Summarizing scores for each specified rater: {RATERS_TO_USE}...")
    if 'Source' not in df_merged_all.columns:
        logging.error("Cannot calculate scores: 'Source' column missing.")
        return None, df_merged_all # Return original df if source missing

    # Columns to summarize are just the original rater columns
    cols_to_summarize = RATING_COLS_TO_PROCESS

    if not cols_to_summarize:
         logging.error("No rating columns identified to summarize.")
         return None, df_merged_all

    logging.info(f"Summarizing scores by Source for columns: {cols_to_summarize}")
    try:
        # Group by source and aggregate stats for the specified rater columns
        summary_stats = df_merged_all.groupby('Source')[cols_to_summarize].agg(['mean', 'std', 'count'])
        logging.info("Summary Statistics by Source (Per Rater):\n" + summary_stats.round(3).to_string())
        # Return the summary and the original merged dataframe (no new average columns added)
        return summary_stats, df_merged_all
    except Exception as e:
        logging.error(f"Failed to calculate summary statistics by source: {e}", exc_info=True)
        return None, df_merged_all
# --- <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< ---

def calculate_iaa(df_merged_all: pd.DataFrame) -> Dict[str, float]:
    """Calculates IAA between the original raters specified in RATERS_TO_USE."""
    # (Function remains the same as in v3 - it correctly uses RATERS_TO_USE)
    logging.info("Calculating Inter-Annotator Agreement (IAA)...")
    iaa_scores = {}
    if len(RATERS_TO_USE) < 2:
        logging.warning(f"IAA requires >= 2 raters. Currently using: {RATERS_TO_USE}. Skipping.")
        return iaa_scores
    num_raters_in_calc = len(RATERS_TO_USE)
    logging.info(f"Calculating IAA based on Raters: {RATERS_TO_USE}")
    for crit in RATING_CRITERIA:
        logging.info(f"\n--- IAA for Criterion: {crit} ---")
        rater_cols_for_crit = [f"Rater{r}_{crit}" for r in RATERS_TO_USE]
        try:
            df_crit_iaa_data = df_merged_all[rater_cols_for_crit].copy()
            df_crit_iaa_data = df_crit_iaa_data.apply(pd.to_numeric, errors='coerce')
        except Exception as e: logging.error(f"Error preparing data for IAA '{crit}': {e}. Skipping."); continue
        df_crit_complete = df_crit_iaa_data.dropna()
        num_complete_ratings = len(df_crit_complete)
        logging.info(f"Found {num_complete_ratings} items with complete ratings from all {num_raters_in_calc} raters for '{crit}'.")
        if num_complete_ratings < 2: logging.warning(f"Too few complete ratings for '{crit}'. Skipping."); continue
        # Cohen's Kappa
        if num_raters_in_calc == 2 and sklearn_available:
            try:
                r1c, r2c = df_crit_complete.columns[0], df_crit_complete.columns[1]; r1s, r2s = df_crit_complete[r1c].astype(int), df_crit_complete[r2c].astype(int)
                if r1s.nunique()<=1 and r2s.nunique()<=1 and r1s.iloc[0]==r2s.iloc[0]: kappa=1.0; logging.info("Kappa=1.0")
                elif r1s.nunique()<=1 or r2s.nunique()<=1: kappa=0.0; logging.warning("Kappa=0.0")
                else: kappa = cohen_kappa_score(r1s, r2s)
                iaa_scores[f"Cohen_Kappa_{crit}"] = round(kappa, 4); logging.info(f"Cohen's Kappa: {kappa:.4f}")
            except Exception as e: logging.error(f"Error calc Kappa: {e}")
        # Krippendorff's Alpha
        if krippendorff_available:
            try:
                reliability_data = df_crit_complete.astype(int).transpose().values.tolist()
                logging.debug(f"KAlpha data shape for {crit}: ({len(reliability_data)}, {len(reliability_data[0]) if reliability_data else 0})")
                alpha = krippendorff.alpha(reliability_data=reliability_data, level_of_measurement='interval')
                iaa_scores[f"Krippendorff_Alpha_{crit}"] = round(alpha, 4); logging.info(f"Krippendorff's Alpha: {alpha:.4f}")
            except ValueError as ve:
                 if "out-of-domain values" in str(ve):
                      logging.error(f"KAlpha failed for '{crit}' due to out-of-domain values *despite* cleaning.")
                      unique_in_list=set(); [unique_in_list.update(rl) for rl in reliability_data]; logging.error(f"Unique vals passed: {sorted(list(unique_in_list))}")
                 else: logging.error(f"ValueError calc KAlpha: {ve}", exc_info=True)
            except Exception as e: logging.error(f"Unexpected error calc KAlpha: {e}", exc_info=True)
    if not iaa_scores: logging.warning("No IAA scores calculated.")
    else: logging.info("\n--- IAA Summary ---"); [logging.info(f"{k}: {v}") for k,v in iaa_scores.items()]
    return iaa_scores

# --- Main Execution ---
if __name__ == "__main__":
    original_stdout = sys.stdout
    original_stderr = sys.stderr
    logger = None
    run_timestamp = get_timestamp()

    try:
        print("\n--- Starting Human Evaluation Analysis Script (6_v4) ---")

        # Get Paths via GUI
        combined_ratings_path = select_file("Select SINGLE CSV File With Rater Scores", [("Ratings CSV", "*.csv")])
        if not combined_ratings_path: sys.exit("Ratings file selection cancelled.")
        keyed_sheet_path = select_file("Select KEYED Eval Sheet (human_evaluation_sheet_keyed_*.csv)", [("Keyed CSV", "human_evaluation_sheet_keyed_*.csv"), ("CSV", "*.csv")])
        if not keyed_sheet_path: sys.exit("Keyed sheet selection cancelled.")
        output_dir = select_folder("Select OUTPUT directory for Analysis Results")
        if not output_dir: sys.exit("Output directory selection cancelled.")

        # Setup Logging
        os.makedirs(output_dir, exist_ok=True)
        # >>>>>>>> MODIFICATION: Update log filename <<<<<<<<<<
        log_filename = f"script6_human_eval_analysis_log_Raters{'_'.join(map(str,RATERS_TO_USE))}_{run_timestamp}.txt"
        # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
        log_file_path = os.path.join(output_dir, log_filename)
        logger = Logger(log_file_path, original_stdout, original_stderr)
        if logger.log_file is None: sys.exit("Log file error.")
        sys.stdout = logger; sys.stderr = logger

        logging.info(f"--- Script Start: {run_timestamp} ---")
        logging.info(f"Ratings File: {combined_ratings_path}")
        logging.info(f"Keyed Sheet:  {keyed_sheet_path}")
        logging.info(f"Output Dir:   {output_dir}")
        logging.info(f"Log File:     {log_file_path}")
        logging.info(f"Analyzing Raters: {RATERS_TO_USE}")
        logging.info("-" * 30)

        # 1. Load Ratings (incl. validation/cleaning)
        df_ratings = load_combined_ratings(combined_ratings_path)
        if df_ratings is None: sys.exit("Load ratings failed.")

        # 2. Load Keyed Data
        logging.info(f"Loading keyed sheet: {keyed_sheet_path}")
        try:
            df_keyed = pd.read_csv(keyed_sheet_path, usecols=KEYED_SHEET_COLS_NEEDED)
            logging.info(f"Loaded {len(df_keyed)} rows from keyed sheet.")
            if df_keyed.isnull().any().any(): logging.warning("Keyed sheet has NaNs.")
        except Exception as e: logging.error(f"Load keyed sheet failed: {e}", exc_info=True); sys.exit("Load keyed failed.")

        # 3. Merge Data
        logging.info("Merging ratings with source info...")
        try:
            df_merged_all = pd.merge(df_ratings, df_keyed, on=["SampleID", "SubjectID"], how='left', validate="many_to_one")
            logging.info(f"Merged data shape: {df_merged_all.shape}")
            if df_merged_all['Source'].isnull().any(): logging.warning(f"{df_merged_all['Source'].isnull().sum()} rows lack Source after merge.")
            if len(df_merged_all)!=len(df_ratings): logging.warning("Merge changed row count.")
        except Exception as e: logging.error(f"Merge failed: {e}", exc_info=True); sys.exit("Merge failed.")

        # 4. Calculate Scores Per Rater
        summary_stats_df, df_processed = calculate_scores_per_rater(df_merged_all) # df_processed is just df_merged_all now

        # 5. Calculate Inter-Annotator Agreement (between Rater 1 and Rater 2)
        iaa_scores_dict = calculate_iaa(df_processed)

        # 6. Save Results
        logging.info("\n--- Saving Analysis Results ---")
        results_saved_count = 0
        try:
            if summary_stats_df is not None:
                 # >>>>>>>> MODIFICATION: Update summary filename <<<<<<<<<<
                summary_filename = f"script6_human_evaluation_summary_stats_Raters{'_'.join(map(str,RATERS_TO_USE))}_{run_timestamp}.csv"
                # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
                summary_path = os.path.join(output_dir, summary_filename)
                summary_stats_df.to_csv(summary_path, encoding='utf-8')
                logging.info(f"Saved summary stats: {summary_path}")
                results_saved_count += 1

            if iaa_scores_dict:
                # >>>>>>>> MODIFICATION: Update IAA filename <<<<<<<<<<
                iaa_filename = f"script6_human_evaluation_iaa_Raters{'_'.join(map(str,RATERS_TO_USE))}_{run_timestamp}.csv"
                # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
                iaa_path = os.path.join(output_dir, iaa_filename)
                pd.DataFrame.from_dict(iaa_scores_dict, orient='index', columns=['Score']).to_csv(iaa_path, encoding='utf-8')
                logging.info(f"Saved IAA scores: {iaa_path}")
                results_saved_count += 1
            elif len(RATERS_TO_USE) >= 2:
                 logging.warning("IAA scores dictionary was empty. No IAA file saved.")

            if results_saved_count > 0: print(f"\nSUCCESS: Analysis results saved to {output_dir}")
            else: print("\nNOTE: No analysis result files generated.")

        except Exception as e: logging.error(f"Failed to save results: {e}", exc_info=True)

        logging.info(f"\n{'='*15} Script 6_v4 Finished Successfully {'='*15}")

    except SystemExit as e: logging.warning(f"Script exited: {e}")
    except FileNotFoundError as e: logging.error(f"ERROR: File not found: {e}"); print(f"ERROR: File not found: {e}", file=original_stderr)
    except ValueError as e: logging.error(f"ERROR: Data validation: {e}"); print(f"ERROR: Data validation: {e}", file=original_stderr)
    except Exception as main_e: logging.exception(f"!!! UNEXPECTED ERROR: {main_e} !!!"); print(f"!!! UNEXPECTED ERROR: {main_e} !!!", file=original_stderr)
    finally:
        if logger: logger.close()
        sys.stdout = original_stdout; sys.stderr = original_stderr
        print("\n(Logging finished, stdout/stderr restored)")