# -*- coding: utf-8 -*-
"""3b-validate preprocessed files.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19L9GHYxifXNsqJc7jQ4zGr2h-cfaVERD
"""

# -*- coding: utf-8 -*-
"""
3_Model_Preprocessing_GUI_v4_Validation.py

Loads cleaned CSV data selected via GUI, preprocesses it for a specific
sequence-to-sequence Hugging Face model (tokenization, truncation),
saves the processed data, AND reloads/validates the saved data.

Uses Tkinter GUI prompts for selecting input files and output directory.
Logs console output to a timestamped file in the output directory.
"""

import os
import pandas as pd
import numpy as np
import datetime
import sys
import re
from typing import List, Dict, Optional, Union

# --- Import Tkinter ---
# ... (tkinter imports as before) ...
try:
    import tkinter as tk
    from tkinter import filedialog
    tkinter_available = True
except ImportError:
    tkinter_available = False
    # Print directly to original stderr before potential redirection
    print("ERROR: Tkinter library not found. This script requires Tkinter for GUI prompts.", file=sys.stderr)
    sys.exit(1)


# --- Import Hugging Face Libraries ---
try:
    from datasets import load_dataset, DatasetDict, Dataset, Features, Value, load_from_disk # Added load_from_disk
    from transformers import AutoTokenizer
except ImportError:
    print("ERROR: 'datasets' or 'transformers' library not found.", file=sys.stderr)
    print("Please install them: pip install datasets transformers", file=sys.stderr)
    sys.exit(1)

# --- Configuration Constants ---
# ... (Constants remain the same) ...
INPUT_BODY_COL = 'body'
INPUT_SUBJECT_COL = 'subject'
EXPECTED_INPUT_COLUMNS: List[str] = [
    'original_filename', 'original_body', 'original_subject',
    'original_annotation_0', 'original_annotation_1', 'original_annotation_2',
    'body', 'subject', 'annotation_0', 'annotation_1', 'annotation_2'
]
MODEL_CHECKPOINT = "facebook/bart-large"
PREFIX = "summarize: " if "t5" in MODEL_CHECKPOINT else ""
MAX_INPUT_LENGTH = 512
MAX_TARGET_LENGTH = 32
PROCESSED_SUBDIR_NAME = "processed_datasets"


# --- Logger Class ---
# ... (Logger class definition remains the same) ...
class Logger:
    def __init__(self, filepath, original_stdout, original_stderr):
        self.terminal = original_stdout; self.stderr_orig = original_stderr; self.log_file = None
        try:
            self.log_file = open(filepath, "w", encoding='utf-8', errors='replace')
            print(f"Logging console output to: {filepath}") # Prints before redirection
        except Exception as e: print(f"FATAL ERROR: Could not open log file {filepath}: {e}", file=self.stderr_orig); self.log_file = None
    def write(self, message):
        try:
            self.terminal.write(message)
            if self.log_file: self.log_file.write(message)
        except Exception as e: print(f"Logger Write Error: {e}", file=self.stderr_orig)
    def flush(self):
        try:
            self.terminal.flush()
            if self.log_file: self.log_file.flush()
        except Exception as e: print(f"Logger Flush Error: {e}", file=self.stderr_orig)
    def close(self):
        if self.log_file:
            try: self.log_file.close()
            except Exception as e: print(f"Logger Close Error: {e}", file=self.stderr_orig)
            self.log_file = None


# --- Helper Functions ---
# ... (get_timestamp, select_file, select_folder definitions remain the same) ...
def get_timestamp() -> str:
    return datetime.datetime.now().strftime('%Y%m%d_%H%M%S')

def select_file(title: str) -> Optional[str]:
    if not tkinter_available: print("ERROR: Tkinter not available."); return None
    print(f"\nA file selection window should open. Please select the file for: '{title}'")
    root = tk.Tk(); root.withdraw(); root.attributes('-topmost', True)
    file_path = filedialog.askopenfilename(title=title, filetypes=[("Cleaned CSV Files", "*_cleaned_*.csv"), ("CSV Files", "*.csv"), ("All Files", "*.*")])
    root.destroy()
    if file_path: print(f"  -> Selected: {file_path}"); return file_path
    else: print("  -> File selection cancelled or failed."); return None

def select_folder(title: str) -> Optional[str]:
    if not tkinter_available: print("ERROR: Tkinter not available."); return None
    print(f"\nA folder selection window should open. Please select the folder for: '{title}'")
    root = tk.Tk(); root.withdraw(); root.attributes('-topmost', True)
    folder_path = filedialog.askdirectory(title=title, mustexist=True)
    root.destroy()
    if folder_path: print(f"  -> Selected: {folder_path}"); return folder_path
    else: print("  -> Folder selection cancelled or failed."); return None


# --- Preprocessing Function ---
# ... (preprocess_function definition remains the same) ...
def preprocess_function(examples, tokenizer, prefix, max_input_length, max_target_length):
    inputs = [prefix + str(doc) if pd.notna(doc) else prefix + "" for doc in examples[INPUT_BODY_COL]]
    targets = [str(target) if pd.notna(target) else "" for target in examples[INPUT_SUBJECT_COL]]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=False)
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding=False)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs


# --- Main Execution ---
if __name__ == "__main__":
    original_stdout = sys.stdout
    original_stderr = sys.stderr
    logger = None
    processed_data_save_path = None # Initialize save path variable

    try:
        print("\n--- Starting Model Preprocessing Script (GUI Version v4 - Validation Added) ---")

        # --- Get Output Directory First ---
        output_base_dir_path = select_folder("Select Base OUTPUT Folder (for processed data and logs)")
        if not output_base_dir_path: sys.exit("Output folder selection cancelled. Exiting.")

        # --- Setup Output Dir and Logging ---
        os.makedirs(output_base_dir_path, exist_ok=True)
        run_timestamp = get_timestamp()
        log_filename = f"model_preprocessing_log_{run_timestamp}.txt"
        log_file_path = os.path.join(output_base_dir_path, log_filename)

        logger = Logger(log_file_path, original_stdout, original_stderr)
        if logger.log_file is None: sys.exit("Log file could not be opened. Exiting.")

        sys.stdout = logger
        sys.stderr = logger

        # --- Print Config to Log ---
        print("--- Configuration (Logged) ---")
        print(f"Model Checkpoint:  {MODEL_CHECKPOINT}")
        print(f"Input Prefix:      '{PREFIX}'")
        print(f"Max Input Length:  {MAX_INPUT_LENGTH}")
        print(f"Max Target Length: {MAX_TARGET_LENGTH}")
        print(f"Run Timestamp:     {run_timestamp}")
        print("-" * 30)

        # --- Get Input File Paths via GUI ---
        print("\nPlease select the CLEANED CSV files generated by the previous script.")
        train_csv_path = select_file("Select CLEANED Train CSV File")
        if not train_csv_path: sys.exit("Operation cancelled by user.")
        dev_csv_path = select_file("Select CLEANED Dev CSV File")
        if not dev_csv_path: sys.exit("Operation cancelled by user.")
        test_csv_path = select_file("Select CLEANED Test CSV File")
        if not test_csv_path: sys.exit("Operation cancelled by user.")

        print("\n--- Paths Selected (Logged) ---")
        print(f"Cleaned Train CSV: {train_csv_path}")
        print(f"Cleaned Dev CSV:   {dev_csv_path}")
        print(f"Cleaned Test CSV:  {test_csv_path}")
        print(f"Base Output Dir:   {output_base_dir_path}")
        print(f"Log File:          {log_file_path}")
        print("-" * 30)

        # --- Load Cleaned Datasets with Explicit Types ---
        # ... (Loading logic remains the same, using Features) ...
        print("\n--- Loading Cleaned CSV Datasets with Explicit Types ---")
        try:
            data_files = {"train": train_csv_path, "dev": dev_csv_path, "test": test_csv_path}
            for split, path in data_files.items():
                 if path is None or not os.path.exists(path): raise FileNotFoundError(f"Input file not found for '{split}': {path}")
            expected_features = Features({ col: Value('string') for col in EXPECTED_INPUT_COLUMNS })
            print(f"Using explicit features schema: {expected_features}")
            raw_datasets = load_dataset("csv", data_files=data_files, features=expected_features)
            for split in raw_datasets:
                 if INPUT_BODY_COL not in raw_datasets[split].column_names or \
                    INPUT_SUBJECT_COL not in raw_datasets[split].column_names:
                      raise ValueError(f"Loaded split '{split}' missing '{INPUT_BODY_COL}' or '{INPUT_SUBJECT_COL}'.")
            print("Raw datasets loaded successfully:"); print(raw_datasets)
        except Exception as e: print(f"ERROR: Failed to load datasets from CSV: {e}", file=sys.stderr); import traceback; traceback.print_exc(file=sys.stderr); sys.exit(1)


        # --- Filter Missing/Invalid Data ---
        # ... (Filtering logic remains the same) ...
        print("\n--- Filtering out rows with missing/empty cleaned body or subject ---")
        def filter_invalid(example):
            body_ok = isinstance(example.get(INPUT_BODY_COL), str) and len(example[INPUT_BODY_COL].strip()) > 0
            subject_ok = isinstance(example.get(INPUT_SUBJECT_COL), str) and len(example[INPUT_SUBJECT_COL].strip()) > 0
            return body_ok and subject_ok
        original_sizes = {split: len(raw_datasets[split]) for split in raw_datasets}
        try:
            cols_needed_for_processing = list(set(raw_datasets['train'].column_names) & set([INPUT_BODY_COL, INPUT_SUBJECT_COL]))
            if INPUT_BODY_COL not in cols_needed_for_processing or INPUT_SUBJECT_COL not in cols_needed_for_processing: raise ValueError(f"Dataset missing required columns '{INPUT_BODY_COL}' or '{INPUT_SUBJECT_COL}'.")
            print(f"Selecting columns for processing: {cols_needed_for_processing}")
            filtered_datasets_stage1 = raw_datasets.select_columns(cols_needed_for_processing)
            print("Applying filter...")
            processed_datasets = filtered_datasets_stage1.filter(filter_invalid, num_proc=os.cpu_count())
        except Exception as e: print(f"ERROR during filtering: {e}.", file=sys.stderr); sys.exit(1)
        filtered_sizes = {split: len(processed_datasets[split]) for split in processed_datasets}
        print("Dataset sizes after filtering:")
        for split in original_sizes: print(f"  {split}: Original={original_sizes[split]}, Filtered={filtered_sizes[split]}, Removed={original_sizes[split] - filtered_sizes[split]}")
        if filtered_sizes['train'] == 0: print("\nERROR: No valid training data remaining.", file=sys.stderr); sys.exit("No training data.")
        elif filtered_sizes['dev'] == 0 or filtered_sizes['test'] == 0: print("\nWarning: Dev or Test split has no valid data.", file=sys.stderr)


        # --- Load Tokenizer ---
        # ... (Tokenizer loading remains the same) ...
        print(f"\n--- Loading Tokenizer: {MODEL_CHECKPOINT} ---")
        try:
            tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT, use_fast=True)
            print("Tokenizer loaded successfully.")
        except Exception as e: print(f"ERROR: Failed to load tokenizer '{MODEL_CHECKPOINT}': {e}", file=sys.stderr); sys.exit(1)


        # --- Apply Preprocessing ---
        # ... (Preprocessing map call remains the same, using fn_kwargs) ...
        print("\n--- Applying Preprocessing and Tokenization ---")
        fn_kwargs = {"tokenizer": tokenizer, "prefix": PREFIX, "max_input_length": MAX_INPUT_LENGTH, "max_target_length": MAX_TARGET_LENGTH}
        try:
            remove_cols = processed_datasets["train"].column_names
            print(f"Mapping preprocessing function (removing columns: {remove_cols})...")
            tokenized_datasets = processed_datasets.map(preprocess_function, batched=True, remove_columns=remove_cols, num_proc=os.cpu_count(), desc="Running tokenizer on dataset", fn_kwargs=fn_kwargs)
            print("\nTokenization complete."); print("Processed datasets structure:"); print(tokenized_datasets)
        except Exception as e: print(f"ERROR: Failed during .map() preprocessing: {e}", file=sys.stderr); import traceback; traceback.print_exc(file=sys.stderr); sys.exit(1)


        # --- Save Processed Data ---
        processed_data_save_path = os.path.join(output_base_dir_path, PROCESSED_SUBDIR_NAME)
        print(f"\n--- Saving Processed Datasets to Disk ---")
        try:
            os.makedirs(processed_data_save_path, exist_ok=True)
            tokenized_datasets.save_to_disk(processed_data_save_path)
            print(f"Processed datasets successfully saved to: {processed_data_save_path}")
        except Exception as e: print(f"ERROR: Failed to save processed datasets: {e}", file=sys.stderr); sys.exit(1)

        # <<<--- ADDED VALIDATION SECTION --- >>>
        print("\n--- Reloading and Validating Saved Data ---")
        if not os.path.exists(processed_data_save_path):
            print(f"ERROR: Saved data directory not found at {processed_data_save_path}. Cannot validate.", file=sys.stderr)
        else:
            try:
                print("Attempting to reload dataset from disk...")
                reloaded_datasets = load_from_disk(processed_data_save_path)
                print("Successfully reloaded datasets:")
                print(reloaded_datasets)

                print("\n--- Validation Checks ---")
                expected_splits = ["train", "dev", "test"]
                validation_passed = True
                for split in expected_splits:
                    if split not in reloaded_datasets:
                        print(f"ERROR: Missing expected split '{split}' in reloaded data", file=sys.stderr)
                        validation_passed = False
                    else:
                        print(f"Split '{split}' found. Rows: {len(reloaded_datasets[split])} (Original filtered size: {filtered_sizes.get(split, 'N/A')})")
                        if len(reloaded_datasets[split]) != filtered_sizes.get(split):
                             print(f"Warning: Row count mismatch for split '{split}'!", file=sys.stderr)
                             validation_passed = False # Treat count mismatch as failure for safety

                if "train" in reloaded_datasets:
                     expected_features = {'input_ids', 'attention_mask', 'labels'}
                     actual_features = set(reloaded_datasets["train"].column_names)
                     if actual_features == expected_features:
                         print(f"Features are correct: {actual_features}")
                     else:
                         print(f"ERROR: Feature mismatch! Expected {expected_features}, Found {actual_features}", file=sys.stderr)
                         validation_passed = False

                if validation_passed:
                     print("Basic validation checks passed!")
                else:
                     print("!!! Validation checks failed. Please review errors above. !!!", file=sys.stderr)

                # --- Inspect a few examples from reloaded data ---
                print("\n--- Inspecting Examples from Reloaded Data (Decoded) ---")
                # No need to reload tokenizer, it's still in memory
                for split in expected_splits:
                    if split in reloaded_datasets and len(reloaded_datasets[split]) > 0:
                        print(f"\nExample from '{split}' split (reloaded):")
                        try:
                            example = reloaded_datasets[split][0]
                            input_ids = example['input_ids']
                            decoded_input = tokenizer.decode(input_ids, skip_special_tokens=True)
                            print(f"  Decoded Input ({len(input_ids)} tokens): {decoded_input[:200]}...")

                            label_ids = example['labels']
                            label_ids_for_decode = [l if l != -100 else tokenizer.pad_token_id for l in label_ids]
                            decoded_labels = tokenizer.decode(label_ids_for_decode, skip_special_tokens=True)
                            print(f"  Decoded Labels ({len(label_ids)} tokens): {decoded_labels}")
                        except Exception as inspect_e:
                             print(f"  Warning: Could not inspect example from '{split}': {inspect_e}", file=sys.stderr)
                    else:
                         print(f"Split '{split}' is empty or missing in reloaded data.")

            except Exception as e:
                print(f"ERROR: Failed to reload or validate dataset from disk: {e}", file=sys.stderr)
                import traceback
                traceback.print_exc(file=sys.stderr)
        # <<<--- END ADDED VALIDATION SECTION --- >>>

        print(f"\n{'='*15} Script Finished Successfully {'='*15}")

    except SystemExit as e: print(f"Script exited: {e}", file=original_stderr)
    except Exception as main_e:
         print(f"\n!!! UNEXPECTED SCRIPT ERROR: {main_e} !!!", file=sys.stderr)
         import traceback; traceback.print_exc(file=sys.stderr)

    finally:
        # --- Restore stdout/stderr and close log file ---
        if logger: logger.close()
        sys.stdout = original_stdout
        sys.stderr = original_stderr
        print("\n(Logging finished, stdout/stderr restored)") # Goes only to original console