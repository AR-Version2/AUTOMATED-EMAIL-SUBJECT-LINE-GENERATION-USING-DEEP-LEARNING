# -*- coding: utf-8 -*-
"""5a_Generate_Predictions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lAp8gpmHJs8u64ivA74Y-ipywy6NHPIk
"""

# -*- coding: utf-8 -*-
"""
5a_Generate_Predictions.py

Loads a fine-tuned Seq2Seq model and tokenizer.
Loads the cleaned test data (body column).
Generates subject line predictions for the test set.
Saves predictions mapped to filenames.

Uses Tkinter GUI prompts for selecting model dir, input CSV, output dir.
Logs console output to a timestamped file.
"""

import os
import sys
import datetime
import logging
import pandas as pd
import numpy as np
import re
from typing import List, Dict, Optional, Union, Tuple

# --- Import Tkinter ---
try:
    import tkinter as tk
    from tkinter import filedialog
    tkinter_available = True
except ImportError:
    tkinter_available = False
    print("ERROR: Tkinter library not found.", file=sys.stderr)
    sys.exit(1)

# --- Import ML Libraries ---
try:
    import torch
    from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
    from datasets import Dataset # Using Dataset for easier batching
    from tqdm.auto import tqdm
except ImportError:
    print("ERROR: Essential libraries ('torch', 'transformers', 'datasets', 'tqdm') not found.", file=sys.stderr)
    print("Please install them: pip install torch transformers datasets tqdm", file=sys.stderr)
    sys.exit(1)

# --- Configuration Constants ---
# Columns needed from the *cleaned* input CSV
INPUT_FILENAME_COL = 'original_filename'
INPUT_BODY_COL = 'body'

# --- Generation Parameters (Should match/align with training eval) ---
GENERATION_BATCH_SIZE = 8 # Adjust based on GPU memory
NUM_BEAMS = 4
MAX_TARGET_LENGTH = 32
MAX_INPUT_LENGTH = 512
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print("--- Configuration ---")
print(f"Device for Generation: {DEVICE}")
print(f"Generation Batch Size: {GENERATION_BATCH_SIZE}")
print(f"Num Beams: {NUM_BEAMS}")
print(f"Max Target Length: {MAX_TARGET_LENGTH}")
print(f"Max Input Length: {MAX_INPUT_LENGTH}")
print("-" * 30)

# --- Logger Class ---
class Logger:
    # (Logger class definition remains the same as in previous scripts)
    def __init__(self, filepath, original_stdout, original_stderr):
        self.terminal = original_stdout; self.stderr_orig = original_stderr; self.log_file = None
        try:
            self.log_file = open(filepath, "w", encoding='utf-8', errors='replace')
            print(f"Logging console output to: {filepath}")
        except Exception as e: print(f"FATAL ERROR: Could not open log file {filepath}: {e}", file=self.stderr_orig); self.log_file = None
    def write(self, message):
        try: self.terminal.write(message);
        except Exception as e: print(f"Logger Write Error: {e}", file=self.stderr_orig)
        try:
            if self.log_file: self.log_file.write(message)
        except Exception as e: print(f"Logger File Write Error: {e}", file=self.stderr_orig)
    def flush(self):
        try: self.terminal.flush()
        except Exception as e: print(f"Logger Flush Error (Terminal): {e}", file=self.stderr_orig)
        try:
            if self.log_file: self.log_file.flush()
        except Exception as e: print(f"Logger Flush Error (File): {e}", file=self.stderr_orig)
    def close(self):
        if self.log_file:
            try: self.log_file.close()
            except Exception as e: print(f"Logger Close Error: {e}", file=self.stderr_orig)
            self.log_file = None

# --- Helper Functions ---
def get_timestamp() -> str:
    return datetime.datetime.now().strftime('%Y%m%d_%H%M%S')

def select_folder(title: str) -> Optional[str]:
    # (select_folder remains the same)
    if not tkinter_available: print("ERROR: Tkinter not available."); return None
    print(f"\nA folder selection window should open. Please select the folder for: '{title}'")
    root = tk.Tk(); root.withdraw(); root.attributes('-topmost', True)
    folder_path = filedialog.askdirectory(title=title, mustexist=True)
    root.destroy()
    if folder_path: print(f"  -> Selected: {folder_path}"); return folder_path
    else: print("  -> Folder selection cancelled or failed."); return None

def select_file(title: str, filetypes: List[Tuple[str, str]]) -> Optional[str]:
    # (select_file remains the same)
    if not tkinter_available: print("ERROR: Tkinter not available."); return None
    print(f"\nA file selection window should open. Please select the file for: '{title}'")
    root = tk.Tk(); root.withdraw(); root.attributes('-topmost', True)
    file_path = filedialog.askopenfilename(title=title, filetypes=filetypes)
    root.destroy()
    if file_path: print(f"  -> Selected: {file_path}"); return file_path
    else: print("  -> File selection cancelled or failed."); return None

# --- Core Functions ---

@st.cache_resource # If using streamlit context, keep cache; otherwise remove
def load_model_and_tokenizer(model_dir_path: str):
    # (load_model_and_tokenizer remains the same)
    logging.info(f"Attempting to load model and tokenizer from: {model_dir_path}...")
    if not os.path.isdir(model_dir_path): raise FileNotFoundError(f"Model directory not found: {model_dir_path}.")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_dir_path, use_fast=True)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_dir_path)
        model.to(DEVICE); model.eval()
        logging.info(f"Model and tokenizer loaded successfully onto {DEVICE}.")
        return model, tokenizer
    except Exception as e: logging.error(f"Error loading model/tokenizer: {e}", exc_info=True); raise

def load_cleaned_test_data_for_gen(csv_path: str) -> pd.DataFrame:
    """Loads only necessary columns from the cleaned test CSV."""
    logging.info(f"Loading relevant columns from cleaned test data: {csv_path}")
    if not os.path.exists(csv_path): raise FileNotFoundError(f"Cleaned test CSV not found: {csv_path}")
    try:
        # Read only necessary columns to save memory/time
        use_cols = [INPUT_FILENAME_COL, INPUT_BODY_COL]
        df = pd.read_csv(csv_path, usecols=use_cols, low_memory=False)
        logging.info(f"Loaded {len(df)} rows from {os.path.basename(csv_path)}. Shape: {df.shape}")
        if INPUT_FILENAME_COL not in df.columns or INPUT_BODY_COL not in df.columns:
             raise ValueError(f"Input CSV missing required columns: {INPUT_FILENAME_COL} or {INPUT_BODY_COL}")
        # Handle potential NaN values in body
        df[INPUT_BODY_COL] = df[INPUT_BODY_COL].fillna("").astype(str)
        return df
    except Exception as e: logging.error(f"Failed to load/validate test CSV {csv_path}: {e}"); raise

def generate_predictions(model, tokenizer, bodies: List[str], batch_size: int, device: str) -> List[str]:
    # (generate_predictions function remains the same)
    logging.info(f"Starting prediction generation for {len(bodies)} emails (Batch size: {batch_size})...")
    model.to(device); model.eval()
    all_predictions = []
    for i in tqdm(range(0, len(bodies), batch_size), desc="Generating Predictions"):
        batch_bodies = bodies[i : i + batch_size]
        batch_bodies = [str(b) if pd.notna(b) else "" for b in batch_bodies]
        try:
            inputs = tokenizer(batch_bodies, return_tensors="pt", padding=True, truncation=True, max_length=MAX_INPUT_LENGTH).to(device)
            with torch.no_grad():
                outputs = model.generate(**inputs, max_length=MAX_TARGET_LENGTH + 2, num_beams=NUM_BEAMS, early_stopping=True)
            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            all_predictions.extend([pred.strip() for pred in decoded])
        except Exception as e:
            logging.error(f"Error during generation for batch starting at index {i}: {e}")
            all_predictions.extend(["[GENERATION ERROR]"] * len(batch_bodies))
    logging.info(f"Generated {len(all_predictions)} predictions.")
    return all_predictions

# --- Main Execution ---
if __name__ == "__main__":
    original_stdout = sys.stdout
    original_stderr = sys.stderr
    logger = None
    run_timestamp = get_timestamp()

    try:
        print("\n--- Starting Prediction Generation Script (5a) ---")

        # --- Get Paths via GUI ---
        model_path = select_folder("Select Fine-Tuned Model Directory")
        if not model_path: sys.exit("Model directory selection cancelled.")

        cleaned_test_csv_path = select_file("Select CLEANED Test CSV File", [("Cleaned CSV", "*_cleaned_*.csv"), ("CSV", "*.csv")])
        if not cleaned_test_csv_path: sys.exit("Cleaned Test CSV selection cancelled.")

        output_dir = select_folder("Select OUTPUT directory for Prediction File")
        if not output_dir: sys.exit("Output directory selection cancelled.")

        # --- Setup Logging ---
        os.makedirs(output_dir, exist_ok=True)
        log_filename = f"generate_predictions_log_{run_timestamp}.txt"
        log_file_path = os.path.join(output_dir, log_filename)
        logger = Logger(log_file_path, original_stdout, original_stderr)
        if logger.log_file is None: sys.exit("Log file could not be opened.")
        sys.stdout = logger
        sys.stderr = logger

        logging.info(f"--- Script Start: {run_timestamp} ---")
        logging.info("--- Paths Selected ---")
        logging.info(f"Model Path:       {model_path}")
        logging.info(f"Cleaned Test CSV: {cleaned_test_csv_path}")
        logging.info(f"Output Directory: {output_dir}")
        logging.info(f"Log File:         {log_file_path}")
        logging.info("-" * 30)
        logging.info(f"Device: {DEVICE}")
        logging.info("-" * 30)

        # 1. Load Model and Tokenizer
        model, tokenizer = load_model_and_tokenizer(model_path)
        if model is None or tokenizer is None: sys.exit("Exiting due to model/tokenizer load failure.")

        # 2. Load Cleaned Test Data (only needed columns)
        df_test_input = load_cleaned_test_data_for_gen(cleaned_test_csv_path)
        if df_test_input is None or df_test_input.empty: sys.exit("Exiting because cleaned test data failed to load or is empty.")

        # 3. Generate Predictions
        test_bodies = df_test_input[INPUT_BODY_COL].tolist()
        generated_subjects = generate_predictions(model, tokenizer, test_bodies, GENERATION_BATCH_SIZE, DEVICE)

        # 4. Save Raw Predictions
        if len(generated_subjects) == len(df_test_input):
            df_predictions = pd.DataFrame({
                'original_filename': df_test_input[INPUT_FILENAME_COL],
                'generated_subject': generated_subjects
            })
            pred_filename = f"model_test_predictions_{run_timestamp}.csv"
            pred_path = os.path.join(output_dir, pred_filename)
            try:
                df_predictions.to_csv(pred_path, index=False, encoding='utf-8')
                logging.info(f"Saved raw predictions ({len(df_predictions)} records) to: {pred_path}")
                print(f"\nSUCCESS: Predictions saved to {pred_path}") # Also print success to console
            except Exception as e:
                 logging.error(f"ERROR: Could not save predictions to {pred_path}: {e}", exc_info=True)

        else:
            logging.error(f"Length mismatch! Test bodies: {len(df_test_input)}, Generated subjects: {len(generated_subjects)}. Predictions not saved.")

        logging.info(f"\n{'='*15} Script 5a Finished Successfully {'='*15}")

    except SystemExit as e:
        logging.warning(f"Script exited: {e}")
    except FileNotFoundError as e:
         logging.error(f"ERROR: Required file/directory not found: {e}")
         print(f"ERROR: Required file/directory not found: {e}", file=original_stderr)
    except ValueError as e:
         logging.error(f"ERROR: Data validation failed: {e}")
         print(f"ERROR: Data validation failed: {e}", file=original_stderr)
    except Exception as main_e:
         logging.exception(f"!!! UNEXPECTED SCRIPT ERROR: {main_e} !!!")
         print(f"!!! UNEXPECTED SCRIPT ERROR: {main_e} !!!", file=original_stderr)

    finally:
        # --- Restore stdout/stderr and close log file ---
        if logger: logger.close()
        sys.stdout = original_stdout
        sys.stderr = original_stderr
        print("\n(Logging finished, stdout/stderr restored)")