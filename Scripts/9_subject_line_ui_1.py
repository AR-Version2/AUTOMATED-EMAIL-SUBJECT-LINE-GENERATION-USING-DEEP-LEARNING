# -*- coding: utf-8 -*-
"""9. Subject Line - UI_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/162x0ixs1OMX7P1vBn5X14JmS2YrVMYQ_
"""

# -*- coding: utf-8 -*-\
# Note: Removed the potentially problematic multi-line docstring causing SyntaxError

import os
import sys
import logging
import datetime
import re
import streamlit as st # Streamlit import

# =============================================================================
# --- Set Page Config FIRST ---
# This MUST be the first Streamlit command executed in the script.
# =============================================================================
st.set_page_config(page_title="Email Subject Generator", layout="wide")

# --- ML/DL Imports ---
# Place imports *after* set_page_config
try:
    import torch
    from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
    # Note: datasets library is NOT needed for this inference script
except ImportError:
    # It's okay to call st.error AFTER set_page_config
    st.error(
        "ERROR: Essential libraries ('torch', 'transformers') not found.\n"
        "Please install them in your environment:\n"
        "pip install torch transformers"
    )
    st.stop() # Stop execution if libraries are missing

# --- Configuration ---
# !!! IMPORTANT: Update this path to point to your FINE-TUNED model directory !!!
MODEL_PATH = r"C:\Users\anith\Downloads\bart-large-finetune-20250413_174424\checkpoint-3596"

# --- Generation Parameters ---
MAX_INPUT_LENGTH = 512
MAX_TARGET_LENGTH = 32  # Max length of the generated subject
NUM_BEAMS = 4           # Use beam search for potentially better quality

# --- Determine Device ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# --- Basic Logging Setup (Logs to console where streamlit runs) ---
# Configure logging only once
if not logging.getLogger().hasHandlers():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Load Model and Tokenizer ---
@st.cache_resource # Caches the resource across reruns
def load_model_and_tokenizer(model_dir_path: str):
    """Loads the fine-tuned model and tokenizer (cached by Streamlit)."""
    # Log messages go to console where streamlit was run
    logging.info(f"Attempting to load model and tokenizer from: {model_dir_path}...")
    if not os.path.isdir(model_dir_path):
        # Use session state to signal error to the main UI part
        st.session_state['model_load_error'] = f"Model directory not found: {model_dir_path}."
        logging.error(st.session_state['model_load_error'])
        return None, None
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_dir_path)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_dir_path)

        model.to(DEVICE)
        logging.info(f"Model moved to {DEVICE}.")

        model.eval() # Set to evaluation mode
        logging.info("Model and tokenizer loaded successfully.")
        st.session_state['model_load_error'] = None # Clear error on success
        return model, tokenizer
    except Exception as e:
        logging.error(f"Error loading model/tokenizer: {e}", exc_info=True) # Log traceback
        # Store error message in session state
        st.session_state['model_load_error'] = f"Failed to load model/tokenizer. Check console logs. Error: {e}"
        return None, None

# --- Generation Function ---
def generate_subject(body_text: str, model, tokenizer) -> str:
    """Generates a subject line for the given body text."""
    if not body_text or not model or not tokenizer:
        return "[ERROR: Missing input or model/tokenizer not loaded]"

    logging.info(f"Generating subject line for input starting with: {body_text[:100]}...")
    try:
        body_text = re.sub(r'\s+', ' ', body_text).strip()
        inputs = tokenizer(
            [body_text], return_tensors="pt", padding=True,
            truncation=True, max_length=MAX_INPUT_LENGTH
        ).to(DEVICE)
        with torch.no_grad():
            outputs = model.generate(
                **inputs, max_length=MAX_TARGET_LENGTH + 2,
                num_beams=NUM_BEAMS, early_stopping=True
            )
        decoded_subject = tokenizer.decode(outputs[0], skip_special_tokens=True)
        logging.info(f"Generated Subject: {decoded_subject}")
        return decoded_subject.strip()
    except Exception as e:
        logging.error(f"Error during subject generation: {e}", exc_info=True)
        return f"[Error during generation. Check logs.]"

# --- Main Streamlit App Interface ---

# --- 1. Title ---
st.title("ðŸ“§ Email Subject Line Generator")

# --- Load Model ---
# Initialize session state for error tracking if it doesn't exist
if 'model_load_error' not in st.session_state:
    st.session_state['model_load_error'] = None

# Load model using the cached function
model, tokenizer = load_model_and_tokenizer(MODEL_PATH)

# --- UI Branching: Display based on successful model load ---
if model and tokenizer:
    # --- 7. Model Information ---
    model_name_display = os.path.basename(MODEL_PATH)
    st.caption(f"Using model: `{model_name_display}` | Running on: `{DEVICE.upper()}`")
    st.divider()

    # --- 2. Description/Instructions ---
    st.markdown(
        "This tool uses a fine-tuned **BART-large** model to suggest a subject line "
        "based on the email body you provide below."
    )
    st.write("**How to Use:** Enter email text and click 'Generate Subject'.")
    st.write("")

    # --- 3. Input Area ---
    email_body_input = st.text_area(
        "**Enter Email Body Here:**",
        height=275,
        placeholder="Example: Hi Team, Quick reminder..."
    )

    # --- 5. Action Button ---
    col1, col2 = st.columns([1, 3])
    with col1:
        generate_button = st.button("âœ¨ **Generate Subject**", use_container_width=True, type="primary")

    # --- 6. Output Area & 8. Spinner ---
    if generate_button and email_body_input:
        # --- 8. Spinner/Loading Indicator ---
        with st.spinner("ðŸ§  Generating subject line... Please wait."):
            generated_subject = generate_subject(email_body_input, model, tokenizer)

        # --- 6. Output Area ---
        st.subheader("Suggested Subject Line:")
        st.info(f"**{generated_subject}**")

    elif generate_button:
        st.warning("ðŸ¤” Please paste some email body text into the area above first!")

else:
    # --- Display Error if Model Loading Failed ---
    # Check session state for the specific error message
    st.error(
        f"**Model Initialization Failed!**\n\n"
        f"{st.session_state.get('model_load_error', 'An unknown error occurred during model loading.')}\n\n"
        f"**Please Check:**\n"
        f"*   **Path Correctness:** Verify the `MODEL_PATH` in the script (`{MODEL_PATH}`) is exactly correct.\n"
        f"*   **Model Files:** Ensure required files (`pytorch_model.bin` or `model.safetensors`, `config.json`, etc.) are present.\n"
        f"*   **Library Conflicts:** Ensure incompatible libraries (like Keras 3) are resolved.\n"
        f"*   **Console Logs:** Check the console where you ran `streamlit run` for detailed tracebacks."
    )

# --- Footer ---
st.divider()
st.caption("Capstone Project - Deep Learning for NLP")

# --- Instructions to Run (as comments) ---
# 1. Save code as app_v6_config_first.py (or similar)
# 2. Update MODEL_PATH variable.
# 3. Install: pip install streamlit transformers torch pandas numpy
# 4. Run: streamlit run app_v6_config_first.py