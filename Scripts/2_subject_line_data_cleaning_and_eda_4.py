# -*- coding: utf-8 -*-
"""2. Subject Line - Data Cleaning and EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I06iLMPnC3EO_SERESl_F8RIGq8_Hpwq
"""

# -*- coding: utf-8 -*-
"""
2_Subject_Line_Clean_EDA_GUI_v5_Logging.py

Loads sampled CSVs, applies refined cleaning, performs EDA, saves cleaned CSVs
alongside original data. Uses Tkinter GUI prompts for file/folder selection.
Logs all console output (stdout & stderr) to a timestamped file in the output directory.
Designed for local execution on systems with a graphical display.
"""

import os
import re
import pandas as pd
import numpy as np
import datetime
import sys
from typing import List, Dict, Optional, Union, Tuple

# --- Import Tkinter for File/Folder Selection ---
# This part still assumes execution in an environment where Tkinter can run
try:
    import tkinter as tk
    from tkinter import filedialog
    tkinter_available = True
except ImportError:
    tkinter_available = False
    print("Warning: Tkinter library not found. GUI prompts disabled. Script requires modification for path input.", file=sys.stderr)
    # If Tkinter is critical, exit here or implement alternative path input (e.g., argparse)
    # For now, we'll let it potentially fail later if select_ functions are called without Tkinter

# --- Optional Imports for EDA ---
try:
    import nltk
    from nltk.corpus import stopwords
    nltk_available = True
except ImportError:
    nltk_available = False
    print("Info: 'nltk' library not found. Advanced tokenization/stopword removal in EDA will be skipped.")
    # No need to print install instructions here, was done before

from collections import Counter

# --- Configuration Constants ---
MISSING_PLACEHOLDER: str = "0000"
ORIGINAL_COLUMNS: List[str] = ['filename', 'body', 'subject', 'annotation_0', 'annotation_1', 'annotation_2']
FINAL_COLUMN_ORDER: List[str] = [
    'original_filename', 'original_body', 'original_subject',
    'original_annotation_0', 'original_annotation_1', 'original_annotation_2',
    'body', 'subject', 'annotation_0', 'annotation_1', 'annotation_2'
]
BOILERPLATE_CLOSINGS = [
    'thanks', 'thank you', 'regards', 'best regards', 'sincerely',
    'cheers', 'best', 'cordially', 'regards'
]
KNOWN_BOILERPLATE_PHRASES = [
    r'\bthe information contained herein is based on sources\b.*\bdo not represent that it is accurate or complete\b',
    r'\bthis message and any attachments are confidential\b.*\byou have received this message in error\b',
    r'\byou have received this message because someone has attempted to send you an e-mail from outside of enron\b',
    r'\benron benefits department\b',
    r'internet communications cannot be guaranteed to be secure',
]
boilerplate_closing_line_pattern_str = r"^\s*(?:" + "|".join(BOILERPLATE_CLOSINGS) + r"),?\s*[\w\s.-]*\s*$"
BOILERPLATE_CLOSING_LINE_REGEX = re.compile(boilerplate_closing_line_pattern_str, re.IGNORECASE | re.MULTILINE)
simple_end_pattern_str = r"\s*\b(?:" + "|".join(BOILERPLATE_CLOSINGS) + r")\b[,.]?\s*$"
SIMPLE_END_REGEX = re.compile(simple_end_pattern_str, re.IGNORECASE)
signature_pattern_str = r"^\s*[-–—]+\s*[\w\s.-]+$"
SIGNATURE_REGEX = re.compile(signature_pattern_str, re.MULTILINE)
list_marker_pattern_str = r"^\s*(?:\d+\.|\d+\)|\b[a-z]\)|\(\s*[ivx]+\s*\))\s+" # Added number + )
LIST_MARKER_REGEX = re.compile(list_marker_pattern_str, re.IGNORECASE | re.MULTILINE)

# --- Logger Class ---
class Logger:
    """Logs print statements (stdout) and errors (stderr) to both console and file."""
    def __init__(self, filepath, original_stdout, original_stderr):
        self.terminal = original_stdout # The original console stdout
        self.stderr_orig = original_stderr # The original console stderr
        self.log_file = None
        try:
            # Use 'a' append mode if you want to keep logs from multiple runs in one file
            # Use 'w' write mode to overwrite the log file each time
            self.log_file = open(filepath, "w", encoding='utf-8', errors='replace')
            print(f"Logging console output to: {filepath}") # Print using original stdout before redirection
        except Exception as e:
            # Use original stderr to report failure opening log file
            print(f"FATAL ERROR: Could not open log file {filepath}: {e}", file=self.stderr_orig)
            self.log_file = None # Ensure log_file is None if opening failed

    def write(self, message):
        try:
            self.terminal.write(message) # Write to original console
            if self.log_file:
                self.log_file.write(message) # Write to file
        except Exception as e:
             # If writing fails, try writing error to original stderr
             print(f"Logger Write Error: {e}", file=self.stderr_orig)


    def flush(self):
        # This flush method is needed for compatibility.
        try:
            self.terminal.flush()
            if self.log_file:
                self.log_file.flush()
        except Exception as e:
            print(f"Logger Flush Error: {e}", file=self.stderr_orig)


    def close(self):
        """Closes the log file."""
        if self.log_file:
            try:
                self.log_file.close()
            except Exception as e:
                 print(f"Logger Close Error: {e}", file=self.stderr_orig)
            self.log_file = None

# --- NLTK Setup (Optional) ---
def setup_nltk():
    # (Function remains the same)
    if nltk_available:
        print("Checking NLTK resource availability...")
        try: nltk.data.find('tokenizers/punkt'); print("  - NLTK 'punkt' resource found.")
        except LookupError: print("  - NLTK 'punkt' resource not found. Downloading..."); nltk.download('punkt', quiet=True); print("  - NLTK 'punkt' downloaded.")
        try: nltk.data.find('corpora/stopwords'); print("  - NLTK 'stopwords' resource found.")
        except LookupError: print("  - NLTK 'stopwords' resource not found. Downloading..."); nltk.download('stopwords', quiet=True); print("  - NLTK 'stopwords' downloaded.")
        print("NLTK setup check complete.")
    else: print("Skipping NLTK setup (library not installed).")

# --- Helper Functions ---
def get_timestamp() -> str:
    return datetime.datetime.now().strftime('%Y%m%d_%H%M%S')

def select_file(title: str) -> Optional[str]:
    if not tkinter_available:
         print("ERROR: Tkinter not available for GUI file selection.", file=sys.stderr)
         return None
    print(f"\nA file selection window should open. Please select the file for: '{title}'")
    root = tk.Tk(); root.withdraw(); root.attributes('-topmost', True)
    file_path = filedialog.askopenfilename(title=title, filetypes=[("CSV Files", "*.csv"), ("All Files", "*.*")])
    root.destroy()
    if file_path:
        if not file_path.lower().endswith('.csv'): print(f"Warning: Selected file '{os.path.basename(file_path)}' does not have a .csv extension.")
        print(f"  -> Selected: {file_path}"); return file_path
    else: print("  -> File selection cancelled or failed."); return None

def select_folder(title: str) -> Optional[str]:
    if not tkinter_available:
         print("ERROR: Tkinter not available for GUI folder selection.", file=sys.stderr)
         return None
    print(f"\nA folder selection window should open. Please select the folder for: '{title}'")
    root = tk.Tk(); root.withdraw(); root.attributes('-topmost', True)
    folder_path = filedialog.askdirectory(title=title, mustexist=True)
    root.destroy()
    if folder_path: print(f"  -> Selected: {folder_path}"); return folder_path
    else: print("  -> Folder selection cancelled or failed."); return None

def load_and_validate_csv(file_path: str) -> Optional[pd.DataFrame]:
    # (Function remains the same)
    print(f"--> Attempting to load: {file_path}")
    if not os.path.exists(file_path): print(f"--> ERROR: File not found at path: {file_path}", file=sys.stderr); return None
    try:
        print(f"--> File exists. Reading with pandas..."); df = pd.read_csv(file_path, low_memory=False)
        if df.empty:
             if os.path.getsize(file_path) > 50: print(f"--> Warning: Pandas read an empty DataFrame from a non-empty file: {file_path}.")
             else: print(f"--> Info: File '{os.path.basename(file_path)}' is effectively empty.")
             return pd.DataFrame(columns=ORIGINAL_COLUMNS)
        missing_cols = [col for col in ORIGINAL_COLUMNS if col not in df.columns]
        extra_cols = [col for col in df.columns if col not in ORIGINAL_COLUMNS]
        if missing_cols: print(f"--> ERROR: File '{os.path.basename(file_path)}' missing columns: {missing_cols}", file=sys.stderr); return None
        if extra_cols: print(f"--> Warning: File '{os.path.basename(file_path)}' has extra columns, ignoring: {extra_cols}"); df = df[ORIGINAL_COLUMNS]
        print(f"--> Successfully loaded and validated '{os.path.basename(file_path)}'. Shape: {df.shape}")
        return df
    except pd.errors.EmptyDataError: print(f"--> Warning: File '{os.path.basename(file_path)}' is empty (Pandas EmptyDataError)."); return pd.DataFrame(columns=ORIGINAL_COLUMNS)
    except Exception as e: print(f"--> ERROR: Exception during loading/validation of {file_path}: {e}", file=sys.stderr); return None

# --- Text Cleaning Functions ---
def normalize_whitespace(text: str) -> str:
    if not isinstance(text, str): return text
    text = re.sub(r'\s+', ' ', text); return text.strip()

def remove_phone_numbers(text: str) -> str:
    if not isinstance(text, str): return text
    phone_pattern_standard = re.compile(r'(?<!\d)(?:\+?\d{1,3}[-.\s]?)?\(?\d{2,4}\)?[-.\s]?\d{2,4}[-.\s]?\d{3,4}\b(?!\d)(?:\s*(?:ext|x)\.?\s*\d{1,6})?')
    extension_pattern_standalone = re.compile(r'\b(?:ext|x)\.?\s+(\d{4,6})\b')
    processed_text = text
    try:
        processed_text = phone_pattern_standard.sub('', processed_text)
        processed_text = extension_pattern_standalone.sub('', processed_text)
    except Exception: return text
    return processed_text

def remove_email_addresses(text: str) -> str:
    if not isinstance(text, str): return text
    email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,7}\b')
    try: return email_pattern.sub('', text)
    except Exception: return text

def remove_specific_noise(text: str) -> str:
    if not isinstance(text, str): return text
    noise_pattern_etc = r'\b(?:ect|etc)\.?\s*(?:[,\s]+(?:ect|etc)\.?\s*){2,}\b'
    text = re.sub(noise_pattern_etc, ' ', text)
    text = re.sub(r'[-=~*#_]{4,}', ' ', text)
    url_pattern = r'(https?://|www\.)\S+'
    text = re.sub(url_pattern, '', text)
    return text

def remove_simple_boilerplate(text: str) -> str:
    if not isinstance(text, str): return text
    cleaned_text = text
    for phrase_pattern in KNOWN_BOILERPLATE_PHRASES:
        try: cleaned_text = re.sub(phrase_pattern, '', cleaned_text, flags=re.IGNORECASE)
        except Exception as e: print(f"Warning: Regex error on boilerplate phrase '{phrase_pattern}': {e}", file=sys.stderr)
    cleaned_text = BOILERPLATE_CLOSING_LINE_REGEX.sub('', cleaned_text)
    cleaned_text = SIMPLE_END_REGEX.sub('', cleaned_text)
    cleaned_text = SIGNATURE_REGEX.sub('', cleaned_text)
    return cleaned_text

def remove_list_markers(text: str) -> str:
    if not isinstance(text, str): return text
    cleaned_text = LIST_MARKER_REGEX.sub(' ', text)
    return cleaned_text

def clean_text(text: Union[str, float, None]) -> Union[str, None]:
    # (Function remains the same - calls the specific cleaning functions)
    if pd.isna(text): return None
    text_str = str(text)
    text_str = remove_phone_numbers(text_str); text_str = remove_email_addresses(text_str)
    text_str = text_str.lower()
    text_str = remove_specific_noise(text_str); text_str = remove_simple_boilerplate(text_str)
    text_str = remove_list_markers(text_str)
    text_str = normalize_whitespace(text_str)
    return text_str if text_str else None

def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    # (Function remains the same - applies clean_text)
    print("\n--- Applying Refined Cleaning Steps (v5 - PII Removed) ---")
    if df.empty: print("DataFrame is empty, skipping cleaning."); return df.copy()
    df_cleaned_only = pd.DataFrame(index=df.index); df_temp = df.copy()
    print(f"Replacing placeholder '{MISSING_PLACEHOLDER}' with NaN for processing.")
    df_temp.replace(MISSING_PLACEHOLDER, np.nan, inplace=True); df_temp.replace('', np.nan, inplace=True)
    text_columns_to_clean = ['body', 'subject', 'annotation_0', 'annotation_1', 'annotation_2']
    for col in text_columns_to_clean:
        if col in df_temp.columns:
            print(f"  Cleaning column: '{col}'...")
            df_cleaned_only[col] = df_temp[col].apply(clean_text)
        else: print(f"  Column '{col}' not found, skipping.")
    print("--- Refined Cleaning complete. ---"); return df_cleaned_only

# --- EDA Function ---
def perform_basic_eda(df_original: pd.DataFrame, df_cleaned: pd.DataFrame, split_name: str):
    # (Function remains the same)
    print(f"\n--- Basic EDA for {split_name.upper()} Sample ---")
    if df_cleaned.empty: print("Cleaned DataFrame is empty, skipping EDA."); return
    df_original_copy = df_original.copy(); df_original_copy.replace(['', MISSING_PLACEHOLDER], np.nan, inplace=True)
    print("\n1. Missing Value Analysis (Based on Original Data):")
    for col in ORIGINAL_COLUMNS:
        if col in df_original.columns:
            placeholder_count = (df_original[col] == MISSING_PLACEHOLDER).sum()
            nan_count_orig = df_original_copy[col].isna().sum()
            total_missing = nan_count_orig
            missing_percentage = (total_missing / len(df_original_copy)) * 100 if len(df_original_copy) > 0 else 0
            print(f"  '{col}': {missing_percentage:.2f}% missing (incl. '{MISSING_PLACEHOLDER}' or originally empty/NaN)")
    print("\n2. Data Cleaning Examples (First 3 Rows, Cleaned vs Original):")
    for i in range(min(3, len(df_original))):
        print(f"\nRow {i} (Filename: {df_original.iloc[i].get('filename', 'N/A')}):")
        for col in ['body', 'subject']:
             if col in df_original.columns and col in df_cleaned.columns:
                 original_val = df_original.iloc[i].get(col, ''); cleaned_val = df_cleaned.iloc[i].get(col)
                 original_val_disp = original_val if pd.notna(original_val) and original_val != '' else f"[{MISSING_PLACEHOLDER} or Empty]"
                 cleaned_val_disp = cleaned_val if pd.notna(cleaned_val) else "[Removed/NaN]"
                 print(f"  Original {col[:50]}...: {str(original_val_disp)[:150]}...")
                 print(f"  Cleaned  {col[:50]}...: {str(cleaned_val_disp)[:150]}...")
    print("\n3. Text Length Analysis (Word Count, Post-Cleaning):")
    def word_count(text):
        if pd.isna(text) or not isinstance(text, str): return 0
        return len(text.split())
    df_eda = df_cleaned.copy(); has_content = False
    for col in ['body', 'subject']:
        if col in df_eda.columns:
            valid_texts = df_eda[col].dropna()
            if not valid_texts.empty:
                has_content = True; length_col_name = f"{col}_word_count"
                df_eda.loc[valid_texts.index, length_col_name] = valid_texts.apply(word_count)
                print(f"\n  Statistics for '{col}' word count (rows with content):")
                print(df_eda[length_col_name].dropna().describe().to_string())
            else: print(f"\n  No text content found for '{col}' after cleaning.")
    if not has_content: print("No text content found in body or subject columns for length analysis.")
    print("\n4. Basic Vocabulary Analysis (Top 20 Words, Post-Cleaning):")
    stop_words = set()
    if nltk_available:
        try: stop_words = set(stopwords.words('english')); print(f"  (Using {len(stop_words)} NLTK English stopwords)")
        except Exception as e: print(f"  Warning: Could not load NLTK stopwords: {e}.")
    has_vocab = False
    for col in ['body', 'subject']:
        if col in df_cleaned.columns:
            valid_texts = df_cleaned[col].dropna().astype(str)
            if valid_texts.empty: print(f"\n  No text content found for '{col}'. Skipping vocab."); continue
            corpus = " ".join(valid_texts)
            if not corpus.strip(): print(f"\n  Corpus for '{col}' is empty. Skipping vocab."); continue
            has_vocab = True; tokens = re.findall(r'\b[a-z0-9_]+\b', corpus)
            if not tokens: print(f"\n  No tokens extracted for '{col}'."); continue
            if stop_words:
                original_token_count = len(tokens); tokens = [word for word in tokens if word not in stop_words]
                print(f"    ({col}: Removed {original_token_count - len(tokens)} stopwords)")
            if not tokens: print(f"\n  No valid tokens remain for '{col}' after stopword removal."); continue
            word_counts = Counter(tokens)
            print(f"\n  Most Common Words in '{col}' (stopwords {'filtered' if stop_words else 'not filtered'}):")
            for word, count in word_counts.most_common(20): print(f"    '{word}': {count}")
    if not has_vocab: print("No text content found in body or subject columns for vocabulary analysis.")
    print("-" * 30)

# --- Saving Function ---
def save_cleaned_csv(df_original: pd.DataFrame, df_cleaned_data: pd.DataFrame, input_file_path: str, output_dir: str, timestamp: str):
    # (Function remains the same - combines original and cleaned)
    if df_cleaned_data is None: print(f"Skipping save for {os.path.basename(input_file_path)} as cleaned DataFrame is None."); return
    try:
        base_name = os.path.splitext(os.path.basename(input_file_path))[0]
        base_name = re.sub(r'_sample_\d+_\d{8}_\d{6}$', '', base_name); base_name = re.sub(r'_data_\d{8}_\d{6}$', '', base_name); base_name = re.sub(r'_\d{8}_\d{6}$', '', base_name)
        output_csv_filename = f"{base_name}_cleaned_{timestamp}.csv"; output_csv_path = os.path.join(output_dir, output_csv_filename)
        print(f"\nAttempting to save combined original and cleaned data to: {output_csv_path}")
        df_original_renamed = df_original.copy(); df_original_renamed.columns = [f"original_{col}" for col in df_original_renamed.columns if col in ORIGINAL_COLUMNS]
        if 'original_filename' not in df_original_renamed.columns and 'filename' in df_original.columns: df_original_renamed['original_filename'] = df_original['filename']
        df_cleaned_to_concat = df_cleaned_data.copy(); df_cleaned_to_concat.index = df_original_renamed.index
        df_combined = pd.concat([df_original_renamed, df_cleaned_to_concat], axis=1)
        for col in FINAL_COLUMN_ORDER:
            if col not in df_combined.columns: df_combined[col] = ''
        df_to_save = df_combined[FINAL_COLUMN_ORDER].fillna(''); df_to_save.to_csv(output_csv_path, index=False, encoding='utf-8')
        print(f"Successfully saved combined data ({len(df_to_save)} records) to: {output_csv_path}")
    except Exception as e: print(f"ERROR: Could not save combined data for {os.path.basename(input_file_path)} to CSV: {e}", file=sys.stderr)


# --- Main Execution ---
if __name__ == "__main__":
    # --- Store original stdout/stderr ---
    original_stdout = sys.stdout
    original_stderr = sys.stderr
    logger = None # Initialize logger to None

    # --- Get Output Directory First to place log file ---
    try:
        print("\n--- Starting Data Cleaning and EDA Script (v5 - Logging & Refined Cleaning) ---")
        output_dir_path = select_folder("Select OUTPUT Folder for Cleaned CSVs and Log File")
        if not output_dir_path: sys.exit("Output folder selection cancelled. Exiting.")

        # --- Create Output Directory ---
        os.makedirs(output_dir_path, exist_ok=True)
        print(f"Output directory confirmed/created: {output_dir_path}")

        # --- Setup Logging ---
        run_timestamp = get_timestamp()
        log_filename = f"cleaning_eda_log_{run_timestamp}.txt"
        log_file_path = os.path.join(output_dir_path, log_filename)

        # Instantiate Logger - it prints its own status message
        logger = Logger(log_file_path, original_stdout, original_stderr)
        if logger.log_file is None:
             # If logger failed to open file, exit gracefully after message printed by logger
             sys.exit("Log file could not be opened. Exiting.")

        # Redirect stdout and stderr
        sys.stdout = logger
        sys.stderr = logger

        # --- Now print initial messages that will go to log and console ---
        print("--- Configuration Constants (Logged) ---")
        print(f"Missing Value Placeholder: '{MISSING_PLACEHOLDER}'")
        print(f"Expected Original Columns: {ORIGINAL_COLUMNS}")
        print(f"Final Output Columns: {FINAL_COLUMN_ORDER}")
        print(f"Run Timestamp: {run_timestamp}")
        print("-" * 20)

        setup_nltk() # NLTK setup messages will now be logged

        # --- Get Input File Paths via GUI ---
        print("\nPlease select the SAMPLE CSV files generated by the first script.")
        train_csv_path = select_file("Select TRAIN Sample CSV File")
        if not train_csv_path: sys.exit("Operation cancelled by user.")

        dev_csv_path = select_file("Select DEV Sample CSV File")
        if not dev_csv_path: sys.exit("Operation cancelled by user.")

        test_csv_path = select_file("Select TEST Sample CSV File")
        if not test_csv_path: sys.exit("Operation cancelled by user.")

        print("\n--- Paths Selected (Logged) ---")
        print(f"Train Sample CSV: {train_csv_path}")
        print(f"Dev Sample CSV:   {dev_csv_path}")
        print(f"Test Sample CSV:  {test_csv_path}")
        print(f"Output Directory: {output_dir_path}")
        print(f"Log File:         {log_file_path}")
        print("-" * 30)

        files_to_process = [("train", train_csv_path), ("dev", dev_csv_path), ("test", test_csv_path)]

        # --- Process Each File ---
        print("\n--- Starting File Processing Loop ---")
        processing_successful_count = 0
        for split_name, input_path in files_to_process:
            # --- Inner try...except to catch errors during processing of a single file ---
            try:
                print(f"\n{'='*15} Processing {split_name.upper()} File {'='*15}")
                print(f"Input path: {input_path}")

                df_original = load_and_validate_csv(input_path)
                if df_original is None: print(f"Skipping {split_name} due to loading error."); continue
                if df_original.empty:
                     print(f"Skipping {split_name} as loaded file was empty.")
                     empty_original_df = pd.DataFrame(columns=[f"original_{c}" for c in ORIGINAL_COLUMNS])
                     empty_cleaned_df = pd.DataFrame()
                     save_cleaned_csv(empty_original_df, empty_cleaned_df, input_path, output_dir_path, run_timestamp)
                     continue

                processing_successful_count += 1
                df_cleaned_data_only = clean_dataframe(df_original)
                perform_basic_eda(df_original, df_cleaned_data_only, split_name)
                save_cleaned_csv(df_original, df_cleaned_data_only, input_path, output_dir_path, run_timestamp)
            except Exception as loop_e:
                 # Log any unexpected error during the loop for a specific file
                 print(f"\n!!! UNEXPECTED ERROR processing {split_name} file: {loop_e} !!!", file=sys.stderr)
                 import traceback
                 traceback.print_exc(file=sys.stderr) # Print traceback to log/stderr
                 print(f"!!! Attempting to continue with next file... !!!", file=sys.stderr)


        # --- Final Output Messages ---
        print(f"\n{'='*15} Processing Loop Finished {'='*15}")
        if processing_successful_count > 0:
             print(f"Cleaned files ({processing_successful_count} splits processed) saved with timestamp '{run_timestamp}' in: {output_dir_path}")
        else:
             print("No files were successfully processed in this run.")
        print(f"{'='*15} Script Complete {'='*15}")

    finally:
        # --- Restore stdout/stderr and close log file ---
        if logger:
            logger.close() # Close the file handle
        sys.stdout = original_stdout
        sys.stderr = original_stderr
        print("\n(Logging finished, stdout/stderr restored)") # Goes only to original console



