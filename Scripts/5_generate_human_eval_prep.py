# -*- coding: utf-8 -*-
"""5_Generate_Human_Eval_Prep.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VrypXYhbRdE-SaERlsG8joafXhN4r60J

Purpose of this Script:
This script takes your best fine-tuned model and the cleaned test data to:
Generate subject lines for all emails in the test set using the model.
Create structured spreadsheets (CSV files) containing a random sample of emails, comparing the model's output to human-written subjects, ready for your human evaluators.
How to Use the Program:
Prerequisites:
Ensure you have the necessary Python libraries installed: streamlit (needed for tkinter execution locally or used previously), transformers, torch, pandas, numpy, datasets, tqdm. If you ran the previous training script successfully, you likely have these.
Have the directory containing your best fine-tuned model (e.g., .../bart-large-finetune-TIMESTAMP/) saved and accessible.
Have the cleaned test set CSV file (e.g., test_cleaned_*.csv, containing the original_ columns and cleaned body/subject) saved and accessible. This file was generated by Script 2 (2_...Clean_EDA...).
Run the Script:
Open your local command prompt or terminal.
Navigate (cd) to the directory where you saved this script (5_Generate_Human_Eval_Prep.py).
Execute the script using Python:
python 5_Generate_Human_Eval_Prep.py
Use code with caution.
Bash
(Make sure you are using the Python environment where the required libraries are installed).
GUI Prompts: The script will pause and open several pop-up windows using tkinter:
First: "Select Fine-Tuned Model Directory..." - Navigate to and select the folder containing your best saved model (the one with pytorch_model.bin, config.json, etc.).
Second: "Select CLEANED Test CSV File..." - Navigate to and select the specific test_cleaned_*.csv file generated by Script 2.
Third: "Select OUTPUT directory for generated files..." - Choose a folder where you want this script to save its output files (the predictions and the human evaluation sheets). It's recommended to create a new, empty folder for this (e.g., Human_Evaluation_Files).
Execution: Once you select the paths, the script will:
Load the model and tokenizer.
Load the test data.
Generate predictions for all test examples (this might take some time, especially on CPU).
Sample the data.
Structure and save the output files.
Log progress to the console and to a log file within the selected output directory.
Completion: The script will print a "Script Finished Successfully" message when done. The command prompt window might close automatically after this if run directly.
Input Files:
Fine-Tuned Model Directory: The folder saved by trainer.save_model() in the previous training step. Contains model weights, config, tokenizer info.
Cleaned Test Set CSV: The specific test_cleaned_*.csv file generated by Script 2. Crucially, this file must contain the cleaned body column, the original_subject column, the chosen annotation column (e.g., original_annotation_0), and the original_filename column for merging and comparison.
Output Files (Saved in the Output Directory you selected):
Log File: generate_eval_prep_log_{timestamp}.txt
Contains all the messages printed to the console during the script's execution. Useful for checking progress and debugging any errors.
Raw Predictions File: model_test_predictions_{timestamp}.csv
Contains two columns: filename (or original_filename) and generated_subject.
Lists the model's generated subject for every email in the input test CSV. Useful for reference or separate analysis.
Keyed Evaluation Sheet: human_evaluation_sheet_keyed_{timestamp}.csv
Contains the data for the sampled emails, structured for analysis after rating.
Includes columns like SampleID, EmailBody, SubjectID (A, B, C...), SubjectToRate, and the crucial Source column (indicating if SubjectToRate came from 'Model', 'Original', or 'Annotation').
Purpose: Primarily for your use during Step 6 (Analysis) to easily group results by source. Do NOT give this file to your raters.
Rater Input Sheet: human_evaluation_sheet_input_{timestamp}.csv
This is the main file for the human evaluation task.
Contains columns: SampleID, EmailBody, SubjectID, SubjectToRate.
The order of subjects within each SampleID is randomized so raters don't know the source.
Includes empty columns for raters to input their scores (e.g., Rater1_Relevance, Rater1_Conciseness, Rater1_Fluency, Rater2_Relevance, etc., based on NUM_RATERS and RATING_CRITERIA set in the script).
Purpose: Distribute this file (or copies) to your human evaluators along with the rating instructions. They will fill in the empty rating columns.
File Location (Local vs. Google Drive):
This script, using tkinter for file/folder selection, operates entirely on your local machine's file system.
When the pop-up windows appear, you will be selecting directories and files from your local computer (like your C: drive or Downloads folder).
If your model or cleaned test data currently resides only on Google Drive from the Colab training, you must download them to your local machine first OR ensure you have Google Drive for Desktop (or a similar tool) installed and synced so that the Drive paths are accessible locally via a standard file path (like G:\\My Drive\\...).
The output files generated by this script will also be saved to the local output directory you select via the GUI prompt.
"""

# -*- coding: utf-8 -*-
"""
5_Generate_Human_Eval_Prep.py

Loads a fine-tuned Seq2Seq model, generates predictions on the test set,
and prepares structured CSV files for human evaluation.

Uses Tkinter GUI prompts for selecting the model directory, input CSV,
and output directory. Logs console output to a timestamped file.
"""

import os
import sys
import datetime
import logging
import pandas as pd
import numpy as np
import random
import re
from typing import List, Dict, Optional, Union, Tuple

# --- Import Tkinter for File/Folder Selection ---
try:
    import tkinter as tk
    from tkinter import filedialog
    tkinter_available = True
except ImportError:
    tkinter_available = False
    # Print directly as logging might not be set up yet
    print("ERROR: Tkinter library not found. This script requires Tkinter for GUI prompts.", file=sys.stderr)
    sys.exit(1)

# --- Import Hugging Face Libraries ---
try:
    import torch
    from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
    from datasets import Dataset # Using Dataset for easier batching during generation
    from tqdm.auto import tqdm # Progress bars
except ImportError:
    print("ERROR: Essential libraries ('torch', 'transformers', 'datasets', 'tqdm') not found.", file=sys.stderr)
    print("Please install them: pip install torch transformers datasets tqdm", file=sys.stderr)
    sys.exit(1)

# --- Configuration Constants ---
# Columns expected/needed from the *cleaned* input CSV (output of script 2)
# Ensure these column names exactly match your cleaned CSV file
INPUT_FILENAME_COL = 'original_filename' # Assuming filename is stored here
INPUT_BODY_COL = 'body'                 # Cleaned body column
INPUT_SUBJECT_COL = 'original_subject'  # Original subject column as one reference
# Define which annotation column to use as the second human reference point
ANNOTATION_COLUMN_TO_USE = "original_annotation_0"

# Define placeholder for missing subjects
MISSING_PLACEHOLDER = ""

# --- Parameters for Generation & Evaluation Prep ---
# !! MODIFY AS NEEDED !!
SAMPLE_SIZE = 100 # Number of emails to sample for human evaluation
GENERATION_BATCH_SIZE = 8 # Batch size for model.generate() - adjust based on GPU memory
NUM_BEAMS = 4            # Number of beams for beam search generation
# Max length of the generated subject (should match/align with training)
MAX_TARGET_LENGTH = 32
# Max input length for tokenizer (must match preprocessing)
MAX_INPUT_LENGTH = 512
# Names for the rating criteria columns
RATING_CRITERIA = ["Relevance", "Conciseness", "Fluency"]

# --- Determine Device ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print("--- Configuration ---")
print(f"Device for Generation: {DEVICE}")
print(f"Human Eval Sample Size: {SAMPLE_SIZE}")
print(f"Annotation Column Used as Reference: {ANNOTATION_COLUMN_TO_USE}")
print(f"Generation Batch Size: {GENERATION_BATCH_SIZE}")
print(f"Num Beams: {NUM_BEAMS}")
print(f"Max Target Length: {MAX_TARGET_LENGTH}")
print(f"Max Input Length: {MAX_INPUT_LENGTH}")
print("-" * 30)

# --- Logger Class (Copied from previous scripts) ---
class Logger:
    def __init__(self, filepath, original_stdout, original_stderr):
        self.terminal = original_stdout; self.stderr_orig = original_stderr; self.log_file = None
        try:
            self.log_file = open(filepath, "w", encoding='utf-8', errors='replace')
            print(f"Logging console output to: {filepath}") # Prints before redirection
        except Exception as e: print(f"FATAL ERROR: Could not open log file {filepath}: {e}", file=self.stderr_orig); self.log_file = None
    def write(self, message):
        try: self.terminal.write(message);
        except Exception as e: print(f"Logger Write Error: {e}", file=self.stderr_orig)
        try: # Separate try block for file write
            if self.log_file: self.log_file.write(message)
        except Exception as e: print(f"Logger File Write Error: {e}", file=self.stderr_orig)
    def flush(self):
        try: self.terminal.flush()
        except Exception as e: print(f"Logger Flush Error (Terminal): {e}", file=self.stderr_orig)
        try:
            if self.log_file: self.log_file.flush()
        except Exception as e: print(f"Logger Flush Error (File): {e}", file=self.stderr_orig)
    def close(self):
        if self.log_file:
            try: self.log_file.close()
            except Exception as e: print(f"Logger Close Error: {e}", file=self.stderr_orig)
            self.log_file = None

# --- Helper Functions ---
def get_timestamp() -> str:
    return datetime.datetime.now().strftime('%Y%m%d_%H%M%S')

def select_folder(title: str) -> Optional[str]:
    if not tkinter_available: print("ERROR: Tkinter not available."); return None
    print(f"\nA folder selection window should open. Please select the folder for: '{title}'")
    root = tk.Tk(); root.withdraw(); root.attributes('-topmost', True)
    folder_path = filedialog.askdirectory(title=title, mustexist=True)
    root.destroy()
    if folder_path: print(f"  -> Selected: {folder_path}"); return folder_path
    else: print("  -> Folder selection cancelled or failed."); return None

def select_file(title: str, filetypes: List[Tuple[str, str]]) -> Optional[str]:
    if not tkinter_available: print("ERROR: Tkinter not available."); return None
    print(f"\nA file selection window should open. Please select the file for: '{title}'")
    root = tk.Tk(); root.withdraw(); root.attributes('-topmost', True)
    file_path = filedialog.askopenfilename(title=title, filetypes=filetypes)
    root.destroy()
    if file_path: print(f"  -> Selected: {file_path}"); return file_path
    else: print("  -> File selection cancelled or failed."); return None

# --- Core Functions ---

#@st.cache_resource # If running within streamlit, otherwise remove decorator
def load_model_and_tokenizer(model_dir_path: str):
    """Loads the fine-tuned model and tokenizer from the specified path."""
    logging.info(f"Attempting to load model and tokenizer from: {model_dir_path}...")
    if not os.path.isdir(model_dir_path):
        raise FileNotFoundError(f"Model directory not found: {model_dir_path}.")
    try:
        # Force torch usage if both backends are installed, avoids Keras 3 conflict potentially
        tokenizer = AutoTokenizer.from_pretrained(model_dir_path, use_fast=True)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_dir_path, force_download=False, local_files_only=False) # Assuming it's downloaded

        model.to(DEVICE)
        model.eval() # Set to evaluation mode
        logging.info(f"Model and tokenizer loaded successfully onto {DEVICE}.")
        return model, tokenizer
    except Exception as e:
        logging.error(f"Error loading model/tokenizer from {model_dir_path}: {e}", exc_info=True)
        raise # Re-raise the exception after logging

def load_cleaned_test_data(csv_path: str) -> pd.DataFrame:
    """Loads the cleaned test CSV using Pandas."""
    logging.info(f"Loading cleaned test data from: {csv_path}")
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"Cleaned test CSV not found: {csv_path}")
    try:
        df = pd.read_csv(csv_path, low_memory=False)
        logging.info(f"Loaded {len(df)} rows from {os.path.basename(csv_path)}. Shape: {df.shape}")
        # Validate necessary columns
        required_cols = [INPUT_FILENAME_COL, INPUT_BODY_COL, INPUT_SUBJECT_COL, ANNOTATION_COLUMN_TO_USE]
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"Input CSV is missing required columns: {missing_cols}")
        # Handle potential NaN values in body before generation
        df[INPUT_BODY_COL] = df[INPUT_BODY_COL].fillna("") # Replace NaN body with empty string
        logging.info(f"Validated required columns exist: {required_cols}")
        return df
    except Exception as e:
        logging.error(f"Failed to load or validate cleaned test CSV {csv_path}: {e}")
        raise

def generate_predictions(model, tokenizer, bodies: List[str], batch_size: int, device: str) -> List[str]:
    """Generates subject line predictions for a list of email bodies."""
    logging.info(f"Starting prediction generation for {len(bodies)} emails (Batch size: {batch_size})...")
    model.to(device)
    model.eval()

    all_predictions = []
    # Wrap range with tqdm for a progress bar
    for i in tqdm(range(0, len(bodies), batch_size), desc="Generating Predictions"):
        batch_bodies = bodies[i : i + batch_size]
        # Filter out any non-string elements just in case
        batch_bodies = [str(b) if pd.notna(b) else "" for b in batch_bodies]

        try:
            inputs = tokenizer(
                batch_bodies, return_tensors="pt", padding=True,
                truncation=True, max_length=MAX_INPUT_LENGTH
            ).to(device)

            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=MAX_TARGET_LENGTH + 2,
                    num_beams=NUM_BEAMS,
                    early_stopping=True
                )

            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            all_predictions.extend([pred.strip() for pred in decoded])

        except Exception as e:
            logging.error(f"Error during generation for batch starting at index {i}: {e}")
            all_predictions.extend(["[GENERATION ERROR]"] * len(batch_bodies))
            # Optional: continue or break/raise depending on desired behavior

    logging.info(f"Generated {len(all_predictions)} predictions.")
    return all_predictions

def prepare_evaluation_sheets(df_merged: pd.DataFrame, sample_size: int, annotation_col: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """Samples data and prepares DataFrames for human evaluation."""
    logging.info(f"Preparing human evaluation sheets from {len(df_merged)} records...")

    actual_sample_size = min(sample_size, len(df_merged))
    if actual_sample_size <= 0:
        logging.warning("No data available to sample for human evaluation.")
        return pd.DataFrame(), pd.DataFrame() # Return empty DataFrames

    logging.info(f"Sampling {actual_sample_size} examples...")
    # Use random_state for reproducible sampling
    df_sample = df_merged.sample(n=actual_sample_size, random_state=42)
    logging.info(f"Sampled {len(df_sample)} examples.")

    eval_data_list = []
    sample_counter = 0
    for index, row in tqdm(df_sample.iterrows(), total=len(df_sample), desc="Structuring Eval Data"):
        sample_counter += 1
        candidates = []

        # Define subject sources
        sources = {
            "Model": row.get('generated_subject'),
            "Original": row.get(INPUT_SUBJECT_COL),
            "Annotation": row.get(annotation_col)
        }

        # Add candidates if they are valid strings and not placeholders/errors
        unique_subjects_in_sample = set()
        for source_name, subject_text in sources.items():
            if pd.notna(subject_text) and isinstance(subject_text, str):
                cleaned_subject = subject_text.strip()
                if cleaned_subject and cleaned_subject != MISSING_PLACEHOLDER and "[GENERATION ERROR]" not in cleaned_subject:
                    # Check for duplicates within this sample
                    if cleaned_subject not in unique_subjects_in_sample:
                         candidates.append({"Source": source_name, "SubjectToRate": cleaned_subject})
                         unique_subjects_in_sample.add(cleaned_subject)
                    else:
                         logging.debug(f"Duplicate subject '{cleaned_subject}' skipped for SampleID {sample_counter}")

        if not candidates:
             logging.warning(f"Skipping SampleID {sample_counter} (Filename: {row.get(INPUT_FILENAME_COL, 'N/A')}) - no valid/unique subjects found.")
             continue

        # Randomize the order presented to raters
        random.shuffle(candidates)
        subject_letter_map = {i: chr(ord('A') + i) for i in range(len(candidates))}

        # Add one row for each candidate subject for this sample
        for i, candidate in enumerate(candidates):
            eval_data_list.append({
                "SampleID": sample_counter,
                "EmailBody": row.get(INPUT_BODY_COL, "[BODY MISSING]"),
                "SubjectID": subject_letter_map[i], # A, B, C...
                "SubjectToRate": candidate['SubjectToRate'],
                "Source": candidate['Source'] # This column will be dropped for the rater sheet
            })

    if not eval_data_list:
        logging.error("No valid data structured for evaluation sheets.")
        return pd.DataFrame(), pd.DataFrame()

    df_eval_keyed = pd.DataFrame(eval_data_list)
    logging.info(f"Created keyed evaluation data with {len(df_eval_keyed)} rows for {sample_counter} samples.")

    # Create the input sheet for raters
    df_eval_input = df_eval_keyed.drop(columns=['Source'])

    # Add columns for ratings (adjust Rater count as needed)
    # Use NUM_RATERS defined at the top
    for rater_num in range(1, NUM_RATERS + 1):
        for criterion in RATING_CRITERIA:
             df_eval_input[f'Rater{rater_num}_{criterion}'] = "" # Initialize as empty strings

    logging.info("Created input sheet structure for raters.")
    return df_eval_keyed, df_eval_input

# --- Main Execution ---
if __name__ == "__main__":
    original_stdout = sys.stdout
    original_stderr = sys.stderr
    logger = None # Initialize logger
    run_timestamp = get_timestamp()

    try:
        print("\n--- Starting Prediction Generation & Human Eval Prep Script ---")

        # --- Get Paths via GUI ---
        model_path = select_folder("Select Fine-Tuned Model Directory (e.g., ...finetune-TIMESTAMP)")
        if not model_path: sys.exit("Model directory selection cancelled.")

        cleaned_test_csv_path = select_file("Select CLEANED Test CSV File (e.g., test_cleaned_*.csv)", [("Cleaned CSV", "*_cleaned_*.csv"), ("CSV", "*.csv")])
        if not cleaned_test_csv_path: sys.exit("Cleaned Test CSV selection cancelled.")

        output_dir = select_folder("Select OUTPUT directory for generated files (Predictions & Eval Sheets)")
        if not output_dir: sys.exit("Output directory selection cancelled.")

        # --- Setup Logging ---
        os.makedirs(output_dir, exist_ok=True)
        log_filename = f"generate_eval_prep_log_{run_timestamp}.txt"
        log_file_path = os.path.join(output_dir, log_filename)
        logger = Logger(log_file_path, original_stdout, original_stderr)
        if logger.log_file is None: sys.exit("Log file could not be opened.")
        sys.stdout = logger
        sys.stderr = logger

        logging.info(f"--- Script Start: {run_timestamp} ---")
        logging.info("--- Paths Selected ---")
        logging.info(f"Model Path:       {model_path}")
        logging.info(f"Cleaned Test CSV: {cleaned_test_csv_path}")
        logging.info(f"Output Directory: {output_dir}")
        logging.info(f"Log File:         {log_file_path}")
        logging.info("-" * 30)
        logging.info(f"Device: {DEVICE}")
        logging.info("-" * 30)

        # 1. Load Model and Tokenizer
        model, tokenizer = load_model_and_tokenizer(model_path)
        if not model or not tokenizer: # Check if loading failed
             sys.exit("Exiting due to model/tokenizer loading failure.")

        # 2. Load Cleaned Test Data
        df_test = load_cleaned_test_data(cleaned_test_csv_path)
        if df_test is None or df_test.empty:
             sys.exit("Exiting because cleaned test data failed to load or is empty.")

        # 3. Generate Predictions
        # Ensure body column is list of strings
        test_bodies = df_test[INPUT_BODY_COL].fillna("").astype(str).tolist()
        generated_subjects = generate_predictions(model, tokenizer, test_bodies, GENERATION_BATCH_SIZE, DEVICE)

        # 4. Save Raw Predictions
        if len(generated_subjects) == len(df_test):
            # Ensure filename column exists for merging
            if INPUT_FILENAME_COL not in df_test.columns:
                 logging.error(f"Column '{INPUT_FILENAME_COL}' not found in test data. Cannot save predictions mapped to filename.")
            else:
                df_predictions = pd.DataFrame({
                    'filename': df_test[INPUT_FILENAME_COL], # Use the correct filename column
                    'generated_subject': generated_subjects
                })
                pred_filename = f"model_test_predictions_{run_timestamp}.csv"
                pred_path = os.path.join(output_dir, pred_filename)
                df_predictions.to_csv(pred_path, index=False, encoding='utf-8')
                logging.info(f"Saved raw predictions to: {pred_path}")

                # 5. Prepare Human Evaluation Data
                # Merge predictions back (use original_filename as key)
                # Need to make sure the column name matches exactly
                df_test_merged = pd.merge(df_test, df_predictions,
                                           left_on=INPUT_FILENAME_COL, right_on='filename',
                                           how='left')
                if 'filename_y' in df_test_merged.columns: # Drop duplicate filename if merge creates it
                    df_test_merged.drop(columns=['filename_y'], inplace=True)
                if 'filename_x' in df_test_merged.columns and INPUT_FILENAME_COL != 'filename_x':
                     df_test_merged.rename(columns={'filename_x': INPUT_FILENAME_COL}, inplace=True)


                logging.info(f"Merged predictions with test data. Shape: {df_test_merged.shape}")
                if df_test_merged['generated_subject'].isna().any():
                     logging.warning("Some predictions were not merged correctly - check filename consistency.")

                df_eval_keyed, df_eval_input = prepare_evaluation_sheets(
                    df_test_merged,
                    SAMPLE_SIZE,
                    ANNOTATION_COLUMN_TO_USE
                )

                # 6. Save Human Evaluation Sheets
                keyed_filename = f"human_evaluation_sheet_keyed_{run_timestamp}.csv"
                keyed_path = os.path.join(output_dir, keyed_filename)
                df_eval_keyed.to_csv(keyed_path, index=False, encoding='utf-8')
                logging.info(f"Saved KEYED evaluation sheet to: {keyed_path}")

                input_filename = f"human_evaluation_sheet_input_{run_timestamp}.csv"
                input_path = os.path.join(output_dir, input_filename)
                df_eval_input.to_csv(input_path, index=False, encoding='utf-8')
                logging.info(f"Saved INPUT sheet for raters to: {input_path}")
        else:
            logging.error(f"Length mismatch! Test bodies: {len(df_test)}, Generated subjects: {len(generated_subjects)}. Cannot merge/create eval sheets.")

        logging.info(f"\n{'='*15} Script Finished Successfully {'='*15}")

    except SystemExit as e:
        logging.warning(f"Script exited: {e}") # Log sys.exit message
    except FileNotFoundError as e:
         logging.error(f"ERROR: Required file/directory not found: {e}")
         print(f"ERROR: Required file/directory not found: {e}", file=sys.stderr)
    except ValueError as e:
         logging.error(f"ERROR: Data validation failed: {e}")
         print(f"ERROR: Data validation failed: {e}", file=sys.stderr)
    except Exception as main_e:
         logging.exception(f"!!! UNEXPECTED SCRIPT ERROR: {main_e} !!!") # Log traceback
         print(f"!!! UNEXPECTED SCRIPT ERROR: {main_e} !!!", file=sys.stderr)

    finally:
        # --- Restore stdout/stderr and close log file ---
        if logger: logger.close()
        sys.stdout = original_stdout
        sys.stderr = original_stderr
        print("\n(Logging finished, stdout/stderr restored)")