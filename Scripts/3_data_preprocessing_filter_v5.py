# -*- coding: utf-8 -*-
"""3_Data_Preprocessing_Filter_v5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13fxDLNpCBQQ1mrLp9SONjtMIAKf3DufP
"""

# -*- coding: utf-8 -*-
"""
3_Data_Preprocessing_v5_FullFilter.py

Loads CLEANED data (Script 2 output) AND QUALITY ANALYSIS data (Script 7 output).
Merges them.
Applies COMBINED filtering:
    - Length-based filtering (on cleaned body/subject).
    - Removes rows with NaN ROUGE scores (from analysis data).
    - Removes rows with generic subject flag (from analysis data).
Preprocesses the filtered data for BART model (tokenization, truncation)
using the CLEANED body/subject columns.
Saves the final processed data (tokenization outputs only) using the
specified naming convention (BART, counts, timestamp).
Reloads and validates the saved data.

Uses Tkinter GUI prompts for file selection and output dir selection.
Logs console output to a timestamped file.
"""

import os
import pandas as pd
import numpy as np
import datetime
import sys
import re
import logging
from typing import List, Dict, Optional, Union , Tuple

# --- Import Tkinter ---
try:
    import tkinter as tk
    from tkinter import filedialog
    tkinter_available = True
except ImportError:
    tkinter_available = False
    print("ERROR: Tkinter library not found.", file=sys.stderr)
    sys.exit(1)

# --- Import Hugging Face Libraries ---
try:
    from datasets import Dataset, DatasetDict, Features, Value, load_from_disk
    from transformers import AutoTokenizer
except ImportError:
    print("ERROR: 'datasets' or 'transformers' library not found.", file=sys.stderr)
    print("Please install them: pip install datasets transformers", file=sys.stderr)
    sys.exit(1)

# --- Configuration Constants ---
# Key column for merging cleaned and analysis data
MERGE_KEY_COL = 'filename' # Assumes this is unique and present in both files

# Columns for PREPROCESSING (using CLEANED versions from Script 2 output)
INPUT_BODY_COL = 'body'
INPUT_SUBJECT_COL = 'subject'

# Columns required from the CLEANED data file (Script 2 output)
# Need the merge key and the cols to be processed/filtered
CLEANED_COLS_NEEDED = [MERGE_KEY_COL, INPUT_BODY_COL, INPUT_SUBJECT_COL]

# Columns required from the ANALYSIS data file (Script 7 output)
ANALYSIS_COLS_NEEDED = [
     'original_filename',
    'subject_body_rouge1',
    'subject_body_rougeL',
    'FLAG_generic_subject',
    # Add other flags from Script 7 if needed for filtering
    # 'FLAG_body_too_short', # Length flags are re-applied to cleaned data below
    # 'FLAG_subject_too_short_chars'
]

# Columns from Analysis file to check for NaN (indicating ROUGE calc failure)
ROUGE_COLS_TO_CHECK_NAN = ['subject_body_rouge1', 'subject_body_rougeL']

# Model and Tokenization Config (Set for BART)
MODEL_CHECKPOINT = "facebook/bart-large"
MODEL_NAME_TAG = "BART"
PREFIX = ""

# Preprocessing Lengths
MAX_INPUT_LENGTH = 512
MAX_TARGET_LENGTH = 32

# Length Filtering Thresholds (Applied to CLEANED body/subject)
MIN_BODY_CHARS = 30
MAX_BODY_CHARS = 15000
MIN_SUBJECT_CHARS = 3
MAX_SUBJECT_CHARS = 200

# Output Directory Name
PROCESSED_SUBDIR_BASE_NAME = "processed_datasets"

print("--- Configuration ---")
print(f"Model Checkpoint:  {MODEL_CHECKPOINT} ({MODEL_NAME_TAG})")
print(f"Processing Columns: Body='{INPUT_BODY_COL}', Subject='{INPUT_SUBJECT_COL}'")
print(f"Body Char Limits:  Min={MIN_BODY_CHARS}, Max={MAX_BODY_CHARS}")
print(f"Subj Char Limits:  Min={MIN_SUBJECT_CHARS}, Max={MAX_SUBJECT_CHARS}")
print(f"Filtering: Removing rows if Length fails OR ROUGE is NaN OR FLAG_generic_subject is not False.")
print(f"Merge Key: '{MERGE_KEY_COL}'")
print("-" * 30)

# --- Helper Functions ---
def get_timestamp() -> str: return datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
def select_file(title: str, filetypes: List[Tuple[str, str]]) -> Optional[str]:
    # (select_file remains the same)
    if not tkinter_available: print("ERROR: Tkinter not available."); return None
    print(f"\nA file selection window ('{title}')...")
    root = tk.Tk(); root.withdraw(); root.attributes('-topmost', True)
    file_path = filedialog.askopenfilename(title=title, filetypes=filetypes)
    root.destroy()
    if file_path: print(f"  -> Selected: {file_path}"); return file_path
    else: print("  -> Selection cancelled."); return None
def select_folder(title: str) -> Optional[str]:
    # (select_folder remains the same)
    if not tkinter_available: print("ERROR: Tkinter not available."); return None
    print(f"\nA folder selection window ('{title}')...")
    root = tk.Tk(); root.withdraw(); root.attributes('-topmost', True)
    folder_path = filedialog.askdirectory(title=title, mustexist=True)
    root.destroy()
    if folder_path: print(f"  -> Selected: {folder_path}"); return folder_path
    else: print("  -> Selection cancelled."); return None
def setup_logging(log_dir: str, timestamp: str) -> None:
    # (setup_logging remains the same)
    log_filename = f"script3_preprocessing_log_{MODEL_NAME_TAG}_{timestamp}.txt"
    log_file_path = os.path.join(log_dir, log_filename)
    os.makedirs(log_dir, exist_ok=True)
    log_handlers = [logging.FileHandler(log_file_path, encoding='utf-8'), logging.StreamHandler(sys.stdout)]
    for handler in logging.root.handlers[:]: logging.root.removeHandler(handler)
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=log_handlers)
    logging.info(f"Logging initialized. Log file: {log_file_path}")

# --- Preprocessing Function ---
def preprocess_function(examples, tokenizer, prefix, max_input_length, max_target_length):
    # (preprocess_function remains the same)
    inputs = [prefix + str(doc) if pd.notna(doc) else prefix + "" for doc in examples[INPUT_BODY_COL]]
    targets = [str(target) if pd.notna(target) else "" for target in examples[INPUT_SUBJECT_COL]]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=False)
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding=False)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# --- Combined Filtering Function ---
def apply_combined_filters(df_merged: pd.DataFrame, split_name: str) -> pd.DataFrame:
    """Applies length, NaN ROUGE, and Generic Subject filters."""
    initial_count = len(df_merged)
    logging.info(f"[{split_name}] Applying combined filters to {initial_count} merged rows...")

    # 1. Length Filter (on cleaned cols)
    body_len_ok = df_merged[INPUT_BODY_COL].astype(str).str.len().between(MIN_BODY_CHARS, MAX_BODY_CHARS, inclusive='both')
    subject_len_ok = df_merged[INPUT_SUBJECT_COL].astype(str).str.len().between(MIN_SUBJECT_CHARS, MAX_SUBJECT_CHARS, inclusive='both')
    length_mask = body_len_ok & subject_len_ok
    logging.info(f"[{split_name}] Rows passing length filter: {length_mask.sum()}")

    # 2. NaN ROUGE Filter (on cols from analysis file)
    rouge_cols = [col for col in ROUGE_COLS_TO_CHECK_NAN if col in df_merged.columns]
    if not rouge_cols:
         logging.warning(f"[{split_name}] ROUGE columns {ROUGE_COLS_TO_CHECK_NAN} not found in merged data. Skipping NaN ROUGE filter.")
         nan_rouge_mask = pd.Series(True, index=df_merged.index) # Pass all if cols missing
    else:
        # Convert to numeric first, coercing errors just in case
        for col in rouge_cols: df_merged[col] = pd.to_numeric(df_merged[col], errors='coerce')
        # Keep rows where ALL specified ROUGE cols are NOT NaN
        nan_rouge_mask = df_merged[rouge_cols].notna().all(axis=1)
        logging.info(f"[{split_name}] Rows passing NaN ROUGE filter: {nan_rouge_mask.sum()}")


    # 3. Generic Subject Filter (on flag from analysis file)
    generic_flag_col = 'FLAG_generic_subject'
    if generic_flag_col not in df_merged.columns:
        logging.warning(f"[{split_name}] Flag column '{generic_flag_col}' not found. Skipping generic filter.")
        generic_mask = pd.Series(True, index=df_merged.index) # Pass all if col missing
    else:
        # Keep rows where the flag is explicitly False
        # Treat NaN or non-boolean as potentially generic (remove)
        generic_mask = df_merged[generic_flag_col].apply(lambda x: x is False)
        logging.info(f"[{split_name}] Rows passing generic subject filter (flag is False): {generic_mask.sum()}")


    # Combine all masks: Row must pass ALL filters
    final_mask = length_mask & nan_rouge_mask & generic_mask
    df_filtered = df_merged[final_mask]
    final_count = len(df_filtered)
    removed_count = initial_count - final_count
    logging.info(f"[{split_name}] Combined filtering complete. Final rows: {final_count} (Removed: {removed_count})")

    return df_filtered

# --- Main Execution ---
if __name__ == "__main__":
    original_stdout = sys.stdout; original_stderr = sys.stderr
    run_timestamp = get_timestamp(); processed_data_save_path = None

    try:
        print("\n--- Starting Model Preprocessing Script (v5 - Full Filter) ---")
        output_base_dir_path = select_folder("Select Base OUTPUT Folder")
        if not output_base_dir_path: sys.exit("Output folder cancelled.")
        setup_logging(output_base_dir_path, run_timestamp)
        logging.info(f"--- Script Start: {run_timestamp} ---"); logging.info("--- Config ---")
        logging.info(f"Model: {MODEL_CHECKPOINT} ({MODEL_NAME_TAG})")# ... log other config ...
        logging.info("-" * 30)

        cleaned_filetypes = [("Cleaned CSV", "*_cleaned_*recs_*.csv"), ("CSV", "*.csv")]
        analysis_filetypes = [("Analysis CSV", "*_quality_analyzed_*.csv"), ("CSV", "*.csv")]

        # --- Get Input File Pairs ---
        logging.info("Requesting CLEANED and ANALYSIS CSV file paths...")
        train_paths = {}; dev_paths = {}; test_paths = {}

        train_paths['cleaned'] = select_file("Select CLEANED Train CSV (*_cleaned_*recs_*.csv)", cleaned_filetypes)
        if not train_paths['cleaned']: sys.exit("Cancelled.")
        train_paths['analysis'] = select_file("Select ANALYSIS Train CSV (*_quality_analyzed_*.csv)",analysis_filetypes)
        if not train_paths['analysis']: sys.exit("Cancelled.")

        dev_paths['cleaned'] = select_file("Select CLEANED Dev CSV (*_cleaned_*recs_*.csv)", cleaned_filetypes)
        if not dev_paths['cleaned']: sys.exit("Cancelled.")
        dev_paths['analysis'] = select_file("Select ANALYSIS Dev CSV (*_quality_analyzed_*.csv)", analysis_filetypes)
        if not dev_paths['analysis']: sys.exit("Cancelled.")

        test_paths['cleaned'] = select_file("Select CLEANED Test CSV (*_cleaned_*recs_*.csv)", cleaned_filetypes)
        if not test_paths['cleaned']: sys.exit("Cancelled.")
        test_paths['analysis'] = select_file("Select ANALYSIS Test CSV (*_quality_analyzed_*.csv)",analysis_filetypes)
        if not test_paths['analysis']: sys.exit("Cancelled.")

        logging.info("--- Paths Selected ---")
        logging.info(f"Train Cleaned:  {train_paths['cleaned']}")
        logging.info(f"Train Analysis: {train_paths['analysis']}")
        logging.info(f"Dev Cleaned:    {dev_paths['cleaned']}")
        logging.info(f"Dev Analysis:   {dev_paths['analysis']}")
        logging.info(f"Test Cleaned:   {test_paths['cleaned']}")
        logging.info(f"Test Analysis:  {test_paths['analysis']}")
        logging.info(f"Base Output Dir: {output_base_dir_path}")
        logging.info("-" * 30)

        # --- Load, Merge, Filter, and Convert Each Split ---
        all_datasets_filtered = {}
        for split in ['train', 'dev', 'test']:
            logging.info(f"\n--- Processing Split: {split} ---")
            cleaned_path = locals()[f"{split}_paths"]['cleaned']
            analysis_path = locals()[f"{split}_paths"]['analysis']

            try:
                 # Load with Pandas
                 logging.info(f"Loading cleaned data: {os.path.basename(cleaned_path)}")
                 df_cleaned = pd.read_csv(cleaned_path, usecols=CLEANED_COLS_NEEDED, low_memory=False)
                 logging.info(f"Loading analysis data: {os.path.basename(analysis_path)}")
                 df_analysis = pd.read_csv(analysis_path, usecols=ANALYSIS_COLS_NEEDED, low_memory=False)

                 # Merge
                 logging.info(f"Merging {split} data on '{MERGE_KEY_COL}'...")

                 #df_merged = pd.merge(df_cleaned, df_analysis, on=MERGE_KEY_COL, how='left')
                 df_merged = pd.merge(
                      df_cleaned,             # Left DataFrame (has 'filename')
                      df_analysis,            # Right DataFrame (has 'original_filename')
                      left_on='filename',     # Key column in df_cleaned
                      right_on='original_filename', # Key column in df_analysis
                      how='left'              # Keep all rows from df_cleaned
                  )
                 # Optional: Drop the redundant key column from the analysis file after merge
                 if 'original_filename' in df_merged.columns:
                    df_merged.drop(columns=['original_filename'], inplace=True)

                 logging.info(f"Merged shape: {df_merged.shape}. Checking merge success...")
                 # Check if merge introduced NaNs in analysis cols for rows that should have matched
                 if df_merged[ANALYSIS_COLS_NEEDED[1]].isnull().sum() > df_analysis[ANALYSIS_COLS_NEEDED[1]].isnull().sum():
                      logging.warning(f"Merge seems to have failed for some rows in {split} (NaNs introduced). Check keys.")

                 # Apply combined filters
                 df_split_filtered = apply_combined_filters(df_merged, split)

                 if df_split_filtered is None or df_split_filtered.empty:
                     logging.error(f"Filtering resulted in empty dataset for split '{split}'. Stopping.")
                     # Handle error - maybe exit or just skip this split?
                     if split == 'train': sys.exit(f"Train split empty after filtering.")
                     else: all_datasets_filtered[split] = Dataset.from_dict({}); continue # Empty dataset for dev/test

                 # Select only columns needed for tokenization BEFORE converting to Dataset
                 cols_for_tokenization = [INPUT_BODY_COL, INPUT_SUBJECT_COL]
                 df_ready_for_tokenization = df_split_filtered[cols_for_tokenization]

                 # Convert the final filtered Pandas DataFrame to Hugging Face Dataset
                 logging.info(f"Converting filtered {split} DataFrame to Hugging Face Dataset...")
                 all_datasets_filtered[split] = Dataset.from_pandas(df_ready_for_tokenization)
                 logging.info(f"Conversion complete for {split}. Final rows: {len(all_datasets_filtered[split])}")

            except Exception as e:
                logging.error(f"Failed processing split {split}: {e}", exc_info=True)
                sys.exit(f"Processing failed for split: {split}")

        # Create DatasetDict
        final_datasets = DatasetDict(all_datasets_filtered)
        logging.info("\n--- Final Datasets after Filtering and Conversion ---")
        logging.info(final_datasets)
        if final_datasets['train'] == 0: logging.error("Train set is empty!"); sys.exit("Empty train set.")

        # --- Load Tokenizer ---
        logging.info(f"\n--- Loading Tokenizer: {MODEL_CHECKPOINT} ---")
        try: tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT, use_fast=True); logging.info("Tokenizer loaded.")
        except Exception as e: logging.error(f"Load tokenizer failed: {e}", exc_info=True); sys.exit(1)

        # --- Apply Preprocessing (Tokenization) ---
        logging.info("\n--- Applying Preprocessing and Tokenization ---")
        fn_kwargs = {"tokenizer": tokenizer, "prefix": PREFIX, "max_input_length": MAX_INPUT_LENGTH, "max_target_length": MAX_TARGET_LENGTH}
        try:
            # Input dataset now only has 'body' and 'subject', so remove_columns isn't needed IF conversion was correct.
            # However, explicitly removing is safer if conversion added __index_level_0__ etc.
            cols_to_remove = final_datasets["train"].column_names # Should just be body, subject + maybe index
            logging.info(f"Mapping preprocess_function (Targeting columns: {cols_to_remove})...")
            tokenized_datasets = final_datasets.map(
                preprocess_function, batched=True, remove_columns=cols_to_remove,
                num_proc=os.cpu_count(), desc="Running tokenizer", fn_kwargs=fn_kwargs
            )
            logging.info("\nTokenization complete."); logging.info(f"Processed datasets structure:\n{tokenized_datasets}")
        except Exception as e: logging.error(f"Map() preprocessing failed: {e}", exc_info=True); sys.exit(1)

        # --- Construct Output Path ---
        logging.info("\n--- Constructing Output Path ---")
        try:
            train_count=len(tokenized_datasets['train']); dev_count=len(tokenized_datasets['dev']); test_count=len(tokenized_datasets['test'])
            processed_subdir_name = f"{PROCESSED_SUBDIR_BASE_NAME}_{MODEL_NAME_TAG}_{train_count}Tr_{dev_count}Dv_{test_count}Ts_{run_timestamp}"
            processed_data_save_path = os.path.join(output_base_dir_path, processed_subdir_name)
            logging.info(f"Final counts: Train={train_count}, Dev={dev_count}, Test={test_count}")
            logging.info(f"Save path: {processed_data_save_path}")
        except Exception as e: logging.error(f"Path construction failed: {e}", exc_info=True); sys.exit("Path error.")

        # --- Save Processed Data ---
        logging.info(f"\n--- Saving Processed Datasets to Disk ---")
        try: os.makedirs(processed_data_save_path, exist_ok=True); tokenized_datasets.save_to_disk(processed_data_save_path); logging.info(f"Saved successfully: {processed_data_save_path}")
        except Exception as e: logging.error(f"Save failed: {e}", exc_info=True); sys.exit(1)

        # --- Reload and Validate Saved Data ---
        logging.info("\n--- Reloading and Validating Saved Data ---")
        # ... (Validation logic remains the same as v4, using final_filtered_counts implicitly via tokenized_datasets counts) ...
        if not os.path.exists(processed_data_save_path): logging.error(f"Save dir not found: {processed_data_save_path}.")
        else:
            try:
                logging.info("Reloading..."); reloaded_datasets = load_from_disk(processed_data_save_path)
                logging.info("Reloaded datasets:"); logging.info(reloaded_datasets)
                logging.info("\n--- Validation Checks ---")
                validation_passed = True; final_filtered_counts_check = {split: len(tokenized_datasets[split]) for split in tokenized_datasets} # Use counts from tokenized data
                for split in ["train", "dev", "test"]:
                    if split not in reloaded_datasets: logging.error(f"Missing split '{split}'"); validation_passed = False; continue
                    reloaded_count = len(reloaded_datasets[split]); expected_count = final_filtered_counts_check.get(split, -1)
                    logging.info(f"Split '{split}': Reloaded={reloaded_count} (Expected={expected_count})")
                    if reloaded_count != expected_count : logging.warning(f"Row count mismatch!"); validation_passed = False
                if "train" in reloaded_datasets:
                     expected_features={'input_ids','attention_mask','labels'}; actual_features=set(reloaded_datasets["train"].column_names)
                     if actual_features==expected_features: logging.info(f"Features correct: {actual_features}")
                     else: logging.error(f"Feature mismatch! Expected {expected_features}, Found {actual_features}"); validation_passed=False
                if validation_passed: logging.info("Basic validation passed!")
                else: logging.error("!!! Validation failed !!!")
                logging.info("\n--- Inspecting Examples ---")
                for split in ["train", "dev", "test"]: # Inspection logic...
                    if split in reloaded_datasets and len(reloaded_datasets[split]) > 0:
                        logging.info(f"\nExample from '{split}' (reloaded):")
                        try:
                            example=reloaded_datasets[split][0]; input_ids=example['input_ids']; decoded_input=tokenizer.decode(input_ids, skip_special_tokens=True)
                            logging.info(f"  Input ({len(input_ids)} toks): {decoded_input[:200]}...")
                            label_ids=example['labels']; label_ids_for_decode=[l if l!=-100 else tokenizer.pad_token_id for l in label_ids]; decoded_labels=tokenizer.decode(label_ids_for_decode, skip_special_tokens=True)
                            logging.info(f"  Labels ({len(label_ids)} toks): {decoded_labels}")
                        except Exception as inspect_e: logging.warning(f"Inspect failed: {inspect_e}")
                    else: logging.info(f"Split '{split}' empty/missing.")
            except Exception as e: logging.error(f"Reload/validate failed: {e}", exc_info=True)

        logging.info(f"\n{'='*15} Script Finished Successfully {'='*15}")

    except SystemExit as e: logging.warning(f"Script exited: {e}")
    except Exception as main_e: logging.exception(f"!!! UNEXPECTED SCRIPT ERROR: {main_e} !!!")
    finally:
        logging.shutdown(); sys.stdout=original_stdout; sys.stderr=original_stderr
        print("\n(Logging finalized)")