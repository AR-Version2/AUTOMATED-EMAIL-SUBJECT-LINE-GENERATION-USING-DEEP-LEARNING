# -*- coding: utf-8 -*-
"""3. Subject Line - Preprocessing 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s6WEPb907UMRhWyf5KICAK9sKvLOXsRc
"""

# -*- coding: utf-8 -*-
"""
3_Model_Preprocessing_GUI_v3.py

Loads cleaned CSV data selected via GUI, preprocesses it for a specific
sequence-to-sequence Hugging Face model (tokenization, truncation),
and saves the processed data to a directory selected via GUI.

Uses Tkinter GUI prompts for selecting input files and output directory.
Logs console output to a timestamped file in the output directory.

Refinements based on user analysis v4:
- Uses fn_kwargs for map multiprocessing to avoid NameError.
- Uses facebook/bart-large as default model checkpoint.
"""

import os
import pandas as pd
import numpy as np
import datetime
import sys
import re
from typing import List, Dict, Optional, Union

# --- Import Tkinter for File/Folder Selection ---
try:
    import tkinter as tk
    from tkinter import filedialog
    tkinter_available = True
except ImportError:
    tkinter_available = False
    print("ERROR: Tkinter library not found. This script requires Tkinter for GUI prompts.", file=sys.stderr)
    sys.exit(1)

# --- Import Hugging Face Libraries ---
try:
    from datasets import load_dataset, DatasetDict, Dataset, Features, Value
    from transformers import AutoTokenizer
except ImportError:
    print("ERROR: 'datasets' or 'transformers' library not found.", file=sys.stderr)
    print("Please install them: pip install datasets transformers", file=sys.stderr)
    sys.exit(1)

# --- Configuration Constants ---
INPUT_BODY_COL = 'body'
INPUT_SUBJECT_COL = 'subject'
# This list MUST match the columns in your *cleaned* CSV files exactly.
EXPECTED_INPUT_COLUMNS: List[str] = [
    'original_filename', 'original_body', 'original_subject',
    'original_annotation_0', 'original_annotation_1', 'original_annotation_2',
    'body', 'subject', 'annotation_0', 'annotation_1', 'annotation_2'
]

# --- Model & Preprocessing Parameters (!! MODIFY AS NEEDED !!) ---
MODEL_CHECKPOINT = "facebook/bart-large" # Chosen default based on analysis
PREFIX = "summarize: " if "t5" in MODEL_CHECKPOINT else ""
MAX_INPUT_LENGTH = 512
MAX_TARGET_LENGTH = 32
PROCESSED_SUBDIR_NAME = "processed_datasets"

print("--- Configuration ---")
print(f"Model Checkpoint:  {MODEL_CHECKPOINT}")
print(f"Input Prefix:      '{PREFIX}'")
print(f"Max Input Length:  {MAX_INPUT_LENGTH}")
print(f"Max Target Length: {MAX_TARGET_LENGTH}")
print(f"Expected Input CSV Columns: {EXPECTED_INPUT_COLUMNS}")
print(f"Processed Data Subdir: '{PROCESSED_SUBDIR_NAME}'")
print("-" * 30)

# --- Logger Class ---
class Logger:
    """Logs print statements (stdout) and errors (stderr) to both console and file."""
    def __init__(self, filepath, original_stdout, original_stderr):
        self.terminal = original_stdout; self.stderr_orig = original_stderr; self.log_file = None
        try:
            self.log_file = open(filepath, "w", encoding='utf-8', errors='replace')
            print(f"Logging console output to: {filepath}")
        except Exception as e: print(f"FATAL ERROR: Could not open log file {filepath}: {e}", file=self.stderr_orig); self.log_file = None
    def write(self, message):
        try:
            self.terminal.write(message)
            if self.log_file: self.log_file.write(message)
        except Exception as e: print(f"Logger Write Error: {e}", file=self.stderr_orig)
    def flush(self):
        try:
            self.terminal.flush()
            if self.log_file: self.log_file.flush()
        except Exception as e: print(f"Logger Flush Error: {e}", file=self.stderr_orig)
    def close(self):
        if self.log_file:
            try: self.log_file.close()
            except Exception as e: print(f"Logger Close Error: {e}", file=self.stderr_orig)
            self.log_file = None

# --- Helper Functions ---
def get_timestamp() -> str:
    return datetime.datetime.now().strftime('%Y%m%d_%H%M%S')

def select_file(title: str) -> Optional[str]:
    if not tkinter_available: print("ERROR: Tkinter not available."); return None
    print(f"\nA file selection window should open. Please select the file for: '{title}'")
    root = tk.Tk(); root.withdraw(); root.attributes('-topmost', True)
    file_path = filedialog.askopenfilename(title=title, filetypes=[("Cleaned CSV Files", "*_cleaned_*.csv"), ("CSV Files", "*.csv"), ("All Files", "*.*")])
    root.destroy()
    if file_path: print(f"  -> Selected: {file_path}"); return file_path
    else: print("  -> File selection cancelled or failed."); return None

def select_folder(title: str) -> Optional[str]:
    if not tkinter_available: print("ERROR: Tkinter not available."); return None
    print(f"\nA folder selection window should open. Please select the folder for: '{title}'")
    root = tk.Tk(); root.withdraw(); root.attributes('-topmost', True)
    folder_path = filedialog.askdirectory(title=title, mustexist=True)
    root.destroy()
    if folder_path: print(f"  -> Selected: {folder_path}"); return folder_path
    else: print("  -> Folder selection cancelled or failed."); return None

# --- Preprocessing Function ---
# This function definition now explicitly includes the parameters
# that will be passed via fn_kwargs
def preprocess_function(examples, tokenizer, prefix, max_input_length, max_target_length):
    """Tokenizes inputs and targets for a seq2seq model."""
    inputs = [prefix + str(doc) if pd.notna(doc) else prefix + "" for doc in examples[INPUT_BODY_COL]]
    targets = [str(target) if pd.notna(target) else "" for target in examples[INPUT_SUBJECT_COL]]

    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=False)
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding=False)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# --- Main Execution ---
if __name__ == "__main__":
    original_stdout = sys.stdout
    original_stderr = sys.stderr
    logger = None

    try:
        print("\n--- Starting Model Preprocessing Script (GUI Version v3 - fn_kwargs) ---")

        # --- Get Output Directory First ---
        output_base_dir_path = select_folder("Select Base OUTPUT Folder (for processed data and logs)")
        if not output_base_dir_path: sys.exit("Output folder selection cancelled. Exiting.")

        # --- Setup Output Dir and Logging ---
        os.makedirs(output_base_dir_path, exist_ok=True)
        run_timestamp = get_timestamp()
        log_filename = f"model_preprocessing_log_{run_timestamp}.txt"
        log_file_path = os.path.join(output_base_dir_path, log_filename)

        logger = Logger(log_file_path, original_stdout, original_stderr)
        if logger.log_file is None: sys.exit("Log file could not be opened. Exiting.")

        sys.stdout = logger
        sys.stderr = logger

        # --- Print Config to Log ---
        print("--- Configuration (Logged) ---")
        print(f"Model Checkpoint:  {MODEL_CHECKPOINT}")
        print(f"Input Prefix:      '{PREFIX}'")
        print(f"Max Input Length:  {MAX_INPUT_LENGTH}")
        print(f"Max Target Length: {MAX_TARGET_LENGTH}")
        print(f"Run Timestamp:     {run_timestamp}")
        print("-" * 30)

        # --- Get Input File Paths via GUI ---
        print("\nPlease select the CLEANED CSV files generated by the previous script.")
        train_csv_path = select_file("Select CLEANED Train CSV File")
        if not train_csv_path: sys.exit("Operation cancelled by user.")

        dev_csv_path = select_file("Select CLEANED Dev CSV File")
        if not dev_csv_path: sys.exit("Operation cancelled by user.")

        test_csv_path = select_file("Select CLEANED Test CSV File")
        if not test_csv_path: sys.exit("Operation cancelled by user.")

        print("\n--- Paths Selected (Logged) ---")
        print(f"Cleaned Train CSV: {train_csv_path}")
        print(f"Cleaned Dev CSV:   {dev_csv_path}")
        print(f"Cleaned Test CSV:  {test_csv_path}")
        print(f"Base Output Dir:   {output_base_dir_path}")
        print(f"Log File:          {log_file_path}")
        print("-" * 30)

        # --- Load Cleaned Datasets with Explicit Types ---
        print("\n--- Loading Cleaned CSV Datasets with Explicit Types ---")
        try:
            data_files = {"train": train_csv_path, "dev": dev_csv_path, "test": test_csv_path}
            for split, path in data_files.items():
                 if path is None or not os.path.exists(path):
                      raise FileNotFoundError(f"Input file not found or not selected for split '{split}': {path}")

            # Define the expected features and their types explicitly
            # IMPORTANT: Ensure this matches the columns in the _cleaned_ CSVs
            expected_features = Features({
                col: Value('string') for col in EXPECTED_INPUT_COLUMNS
            })
            print(f"Using explicit features schema: {expected_features}")

            raw_datasets = load_dataset("csv", data_files=data_files, features=expected_features)

            for split in raw_datasets:
                 if INPUT_BODY_COL not in raw_datasets[split].column_names or \
                    INPUT_SUBJECT_COL not in raw_datasets[split].column_names:
                      raise ValueError(f"Loaded split '{split}' missing '{INPUT_BODY_COL}' or '{INPUT_SUBJECT_COL}'.")

            print("Raw datasets loaded successfully:")
            print(raw_datasets)
        except FileNotFoundError as e: print(f"ERROR: {e}", file=sys.stderr); sys.exit(1)
        except ValueError as e: print(f"ERROR: {e}", file=sys.stderr); sys.exit(1)
        except Exception as e: print(f"ERROR: Failed to load datasets from CSV: {e}", file=sys.stderr); import traceback; traceback.print_exc(file=sys.stderr); sys.exit(1)

        # --- Filter Missing/Invalid Data ---
        print("\n--- Filtering out rows with missing/empty cleaned body or subject ---")
        def filter_invalid(example):
            body_ok = isinstance(example.get(INPUT_BODY_COL), str) and len(example[INPUT_BODY_COL].strip()) > 0
            subject_ok = isinstance(example.get(INPUT_SUBJECT_COL), str) and len(example[INPUT_SUBJECT_COL].strip()) > 0
            return body_ok and subject_ok

        original_sizes = {split: len(raw_datasets[split]) for split in raw_datasets}
        try:
            # Select only needed columns *before* filtering
            cols_needed_for_processing = list(set(raw_datasets['train'].column_names) & set([INPUT_BODY_COL, INPUT_SUBJECT_COL]))
            if INPUT_BODY_COL not in cols_needed_for_processing or INPUT_SUBJECT_COL not in cols_needed_for_processing:
                 raise ValueError(f"Cannot proceed: Input dataset missing required columns '{INPUT_BODY_COL}' or '{INPUT_SUBJECT_COL}' after loading.")

            print(f"Selecting columns for processing: {cols_needed_for_processing}")
            filtered_datasets_stage1 = raw_datasets.select_columns(cols_needed_for_processing)

            print("Applying filter...")
            processed_datasets = filtered_datasets_stage1.filter(filter_invalid, num_proc=os.cpu_count()) # Use multiple cores
        except Exception as e:
            print(f"ERROR during filtering: {e}. Check dataset structure.", file=sys.stderr)
            sys.exit(1)

        filtered_sizes = {split: len(processed_datasets[split]) for split in processed_datasets}
        print("Dataset sizes after filtering:")
        for split in original_sizes: print(f"  {split}: Original={original_sizes[split]}, Filtered={filtered_sizes[split]}, Removed={original_sizes[split] - filtered_sizes[split]}")
        if filtered_sizes['train'] == 0: print("\nERROR: No valid training data remaining after filtering.", file=sys.stderr); sys.exit("No training data.")
        elif filtered_sizes['dev'] == 0 or filtered_sizes['test'] == 0: print("\nWarning: Dev or Test split has no valid data after filtering.", file=sys.stderr)


        # --- Load Tokenizer ---
        print(f"\n--- Loading Tokenizer: {MODEL_CHECKPOINT} ---")
        try:
            tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT, use_fast=True)
            print("Tokenizer loaded successfully.")
        except Exception as e: print(f"ERROR: Failed to load tokenizer '{MODEL_CHECKPOINT}': {e}", file=sys.stderr); sys.exit(1)

        # --- Apply Preprocessing ---
        print("\n--- Applying Preprocessing and Tokenization ---")
        # Define arguments to pass to worker processes using fn_kwargs
        fn_kwargs = {
            "tokenizer": tokenizer,
            "prefix": PREFIX,
            "max_input_length": MAX_INPUT_LENGTH,
            "max_target_length": MAX_TARGET_LENGTH
        }
        try:
            remove_cols = processed_datasets["train"].column_names
            print(f"Mapping preprocessing function (removing columns: {remove_cols})...")
            tokenized_datasets = processed_datasets.map(
                preprocess_function, # Pass function directly
                batched=True,
                remove_columns=remove_cols,
                num_proc=os.cpu_count(), # Use multiple cores
                desc="Running tokenizer on dataset",
                fn_kwargs=fn_kwargs # Pass other args via fn_kwargs
            )
            print("\nTokenization complete."); print("Processed datasets structure:"); print(tokenized_datasets)
        except Exception as e: print(f"ERROR: Failed during .map() preprocessing: {e}", file=sys.stderr); import traceback; traceback.print_exc(file=sys.stderr); sys.exit(1)


        # --- Inspect Example ---
        print("\n--- Inspecting First Training Example ---")
        try:
            if len(tokenized_datasets["train"]) > 0:
                example = tokenized_datasets["train"][0]
                print(f"Input IDs (sample): {example['input_ids'][:50]}...")
                print(f"Decoded Input: {tokenizer.decode(example['input_ids'], skip_special_tokens=True)}")
                print(f"\nLabel IDs (sample): {example['labels'][:50]}...")
                label_ids_for_decode = [l if l != -100 else tokenizer.pad_token_id for l in example["labels"]]
                print(f"Decoded Labels: {tokenizer.decode(label_ids_for_decode, skip_special_tokens=True)}")
            else: print("Training set is empty after processing.")
        except Exception as e: print(f"Warning: Could not inspect example: {e}", file=sys.stderr)


        # --- Save Processed Data ---
        processed_data_save_path = os.path.join(output_base_dir_path, PROCESSED_SUBDIR_NAME)
        print(f"\n--- Saving Processed Datasets to Disk ---")
        try:
            os.makedirs(processed_data_save_path, exist_ok=True) # Ensure subdir exists
            tokenized_datasets.save_to_disk(processed_data_save_path)
            print(f"Processed datasets successfully saved to: {processed_data_save_path}")
        except Exception as e: print(f"ERROR: Failed to save processed datasets: {e}", file=sys.stderr); sys.exit(1)

        print(f"\n{'='*15} Script Finished Successfully {'='*15}")

    except SystemExit as e: print(f"Script exited: {e}", file=original_stderr) # Log exit message
    except Exception as main_e:
         print(f"\n!!! UNEXPECTED SCRIPT ERROR: {main_e} !!!", file=sys.stderr)
         import traceback; traceback.print_exc(file=sys.stderr)

    finally:
        # --- Restore stdout/stderr and close log file ---
        if logger: logger.close()
        sys.stdout = original_stdout
        sys.stderr = original_stderr
        print("\n(Logging finished, stdout/stderr restored)") # Goes only to original console