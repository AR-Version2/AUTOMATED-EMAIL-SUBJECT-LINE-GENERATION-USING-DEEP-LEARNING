# -*- coding: utf-8 -*-
"""7_Analyze_Data_Quality_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m4Q4hQb2a4IP-AZwihYQQ-3k86Tf53eq
"""

# -*- coding: utf-8 -*-
"""
7_Analyze_Data_Quality_v3.py

Analyzes cleaned datasets (train, dev, test) to identify potential quality issues
in email bodies and reference subjects based on length, lexical overlap (ROUGE),
generic terms, and missing values.

Outputs annotated CSV files and a summary report.

v3: Corrects ROUGE calculation to process items individually, fixing the
    'float object is not iterable' error and enabling per-item flagging.
    NOTE: This will be slower than batch processing.
"""

import os
import sys
import datetime
import logging
import pandas as pd
import numpy as np
import nltk
import evaluate # Requires datasets, transformers, rouge-score
import re
from typing import List, Dict, Optional, Union, Tuple
import time

# --- Import Tkinter ---
try:
    import tkinter as tk
    from tkinter import filedialog
    tkinter_available = True
except ImportError:
    tkinter_available = False
    print("ERROR: Tkinter library not found.", file=sys.stderr)
    sys.exit(1)

# --- Configuration ---

# --- Column Names (Ensure these match your cleaned CSVs) ---
FILENAME_COL = 'original_filename' # Or whatever identifies the email
BODY_COL = 'body'
SUBJECT_COL = 'original_subject' # Use the 'original' subject as the reference
ANNOTATION_COLS = ['original_annotation_0'] # Example: Use first annotation

# --- Analysis Thresholds (ADJUST AS NEEDED) ---
MIN_BODY_WORDS = 10
MAX_BODY_WORDS = 1500
MIN_SUBJECT_WORDS = 1
MAX_SUBJECT_WORDS = 20
MIN_SUBJECT_CHARS = 3
MIN_SUBJECT_BODY_ROUGE1 = 0.05
MIN_SUBJECT_BODY_ROUGEL = 0.05

GENERIC_SUBJECT_TERMS = [ # EXPAND THIS LIST
    'hi', 'hello', 'question', 'update', 'meeting', 'follow up', 'quick question',
    'check in', 'checking in', 'fyi', 'info', 're:', 'fw:', 'request',
    'important', 'action required', 'response needed', 'thanks', 'thank you',
    '(no subject)', '', 'test'
]
MISSING_PLACEHOLDER = "0000" # Example if you used this

# --- ROUGE calculation batch size (No longer used for compute, but kept for context) ---
# ROUGE_BATCH_SIZE = 64 # ROUGE is now calculated individually

print("--- Configuration (v3) ---")
print(f"Body Length Flags: <{MIN_BODY_WORDS} or >{MAX_BODY_WORDS} words")
print(f"Subject Length Flags: <{MIN_SUBJECT_WORDS} or >{MAX_SUBJECT_WORDS} words, <{MIN_SUBJECT_CHARS} chars")
print(f"Subject-Body ROUGE Flags: ROUGE-1 < {MIN_SUBJECT_BODY_ROUGE1} or ROUGE-L < {MIN_SUBJECT_BODY_ROUGEL}")
print(f"Generic Subject Terms (Examples): {GENERIC_SUBJECT_TERMS[:5]}...")
print("-" * 30)

# --- Logger Class ---
class Logger: pass # Standard logging used

# --- Helper Functions ---
def get_timestamp() -> str: return datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
def select_folder(title: str) -> Optional[str]:
    if not tkinter_available: print("ERROR: Tkinter unavailable."); return None
    print(f"\nA folder selection window ('{title}')..."); root = tk.Tk(); root.withdraw(); root.attributes('-topmost', True)
    folder_path = filedialog.askdirectory(title=title, mustexist=True); root.destroy()
    if folder_path: print(f"  -> Selected: {folder_path}"); return folder_path
    else: print("  -> Selection cancelled."); return None
def select_files(title: str, filetypes: List[Tuple[str, str]]) -> Optional[List[str]]:
    if not tkinter_available: print("ERROR: Tkinter unavailable."); return None
    print(f"\nA file selection window ('{title}'). Select ALL cleaned CSVs..."); root = tk.Tk(); root.withdraw(); root.attributes('-topmost', True)
    file_paths = filedialog.askopenfilenames(title=title, filetypes=filetypes); root.destroy()
    if file_paths:
        file_list = list(file_paths); print(f"  -> Selected {len(file_list)} files:"); [print(f"     - {os.path.basename(f)}") for f in file_list]; return file_list
    else: print("  -> Selection cancelled."); return None

# --- NLTK and ROUGE Setup ---
rouge_metric = None
nltk_punkt_downloaded = False
try:
    logging.info("Downloading NLTK 'punkt'...")
    nltk.download('punkt', quiet=True)
    nltk_punkt_downloaded = True
    logging.info("NLTK 'punkt' downloaded/verified.")
    logging.info("Loading ROUGE metric...")
    rouge_metric = evaluate.load('rouge')
    logging.info("ROUGE metric loaded successfully.")
except Exception as e:
    logging.error(f"Failed NLTK download or ROUGE metric load: {e}. ROUGE analysis will be skipped.", exc_info=True)
    rouge_metric = None

# --- Core Analysis Functions ---

def load_cleaned_data(csv_path: str) -> Optional[pd.DataFrame]:
    # (Function remains the same)
    logging.info(f"Loading cleaned data from: {csv_path}")
    if not os.path.exists(csv_path): logging.error(f"File not found: {csv_path}"); return None
    try:
        df = pd.read_csv(csv_path, low_memory=False); logging.info(f"Loaded {len(df)} rows from {os.path.basename(csv_path)}. Shape: {df.shape}")
        required_cols = [FILENAME_COL, BODY_COL, SUBJECT_COL] + ANNOTATION_COLS; missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols: logging.error(f"File {csv_path} missing required columns: {missing_cols}"); return None
        df[BODY_COL] = df[BODY_COL].fillna('').astype(str); df[SUBJECT_COL] = df[SUBJECT_COL].fillna('').astype(str)
        for col in ANNOTATION_COLS: df[col] = df[col].fillna('').astype(str)
        logging.info(f"Validated required columns in {os.path.basename(csv_path)}.")
        return df
    except Exception as e: logging.error(f"Failed to load/validate CSV {csv_path}: {e}", exc_info=True); return None

def calculate_lengths(df: pd.DataFrame) -> pd.DataFrame:
    # (Function remains the same)
    logging.info("Calculating text lengths...");
    try:
        df[BODY_COL]=df[BODY_COL].astype(str); df[SUBJECT_COL]=df[SUBJECT_COL].astype(str)
        df['body_word_count'] = df[BODY_COL].apply(lambda x: len(x.split())); df['subject_word_count'] = df[SUBJECT_COL].apply(lambda x: len(x.split()))
        df['body_char_count'] = df[BODY_COL].str.len(); df['subject_char_count'] = df[SUBJECT_COL].str.len(); logging.info("Length calculations complete.")
    except Exception as e: logging.error(f"Error during length calc: {e}", exc_info=True)
    return df

def flag_length_outliers(df: pd.DataFrame) -> pd.DataFrame:
    # (Function remains the same)
    logging.info("Flagging length outliers...")
    df['FLAG_body_too_short'] = df['body_word_count'] < MIN_BODY_WORDS; df['FLAG_body_too_long'] = df['body_word_count'] > MAX_BODY_WORDS
    df['FLAG_subject_too_short_words'] = df['subject_word_count'] < MIN_SUBJECT_WORDS; df['FLAG_subject_too_long_words'] = df['subject_word_count'] > MAX_SUBJECT_WORDS
    df['FLAG_subject_too_short_chars'] = df['subject_char_count'] < MIN_SUBJECT_CHARS; logging.info("Length outlier flagging complete.")
    return df

# --- >>> CORRECTED ROUGE FUNCTION v3 <<< ---
def calculate_subject_body_rouge(df: pd.DataFrame) -> pd.DataFrame:
    """Calculates ROUGE between subject and body individually per row."""
    if rouge_metric is None:
        logging.warning("ROUGE metric not loaded. Skipping subject-body ROUGE calculation.")
        df['subject_body_rouge1'] = np.nan
        df['subject_body_rougeL'] = np.nan
        df['FLAG_low_sb_rouge'] = False
        return df

    logging.info(f"Calculating Subject-Body ROUGE scores (Individually - this may take time)...")
    subjects = df[SUBJECT_COL].tolist()
    bodies = df[BODY_COL].tolist()
    rouge1_scores = []
    rougeL_scores = []
    total_items = len(df)
    start_time = time.time()
    processed_count = 0

    # --- Loop through each item individually ---
    for i in range(total_items):
        subject = str(subjects[i]).strip()
        body = str(bodies[i]).strip()

        # Skip if subject or body is empty after stripping
        if not subject or not body:
            rouge1_scores.append(np.nan)
            rougeL_scores.append(np.nan)
            processed_count += 1
            continue

        try:
            # Calculate ROUGE for a single pair
            results = rouge_metric.compute(predictions=[subject], references=[body], use_stemmer=True)
            r1 = results.get('rouge1')
            rl = results.get('rougeL')
            rouge1_scores.append(round(r1, 4) if isinstance(r1, (int, float)) else np.nan)
            rougeL_scores.append(round(rl, 4) if isinstance(rl, (int, float)) else np.nan)

        except Exception as e:
            if i < 5 or processed_count % 500 == 0: # Log periodically
                 logging.error(f"Error calculating ROUGE for item index {i}: {e}", exc_info=False)
            rouge1_scores.append(np.nan)
            rougeL_scores.append(np.nan)

        processed_count += 1
        if processed_count % 500 == 0: # Log progress
            elapsed = time.time() - start_time
            rate = processed_count / elapsed if elapsed > 0 else 0
            logging.info(f"  Processed {processed_count}/{total_items} items ({elapsed:.1f}s elapsed, {rate:.1f} items/sec)")

    df['subject_body_rouge1'] = rouge1_scores
    df['subject_body_rougeL'] = rougeL_scores
    logging.info(f"Subject-Body ROUGE calculation finished in {time.time() - start_time:.1f} seconds.")

    # Flag low overlap
    df['FLAG_low_sb_rouge'] = (df['subject_body_rouge1'].fillna(1.0) < MIN_SUBJECT_BODY_ROUGE1) | \
                              (df['subject_body_rougeL'].fillna(1.0) < MIN_SUBJECT_BODY_ROUGEL)
    logging.info("Low Subject-Body ROUGE flagging complete.")
    return df
# --- >>> END CORRECTED ROUGE FUNCTION <<< ---

def identify_generic_subjects(df: pd.DataFrame) -> pd.DataFrame:
    # (Function remains the same)
    logging.info("Identifying generic subjects...")
    try:
        def normalize(text): text=str(text).lower().strip(); text=re.sub(r'[^\w\s]','',text); return text.strip()
        generic_set=set(GENERIC_SUBJECT_TERMS); df['FLAG_generic_subject']=df[SUBJECT_COL].apply(lambda x: normalize(x) in generic_set)
        logging.info("Generic subject identification complete.")
    except Exception as e: logging.error(f"Error identifying generic subjects: {e}", exc_info=True); df['FLAG_generic_subject'] = False
    return df

def identify_missing_placeholders(df: pd.DataFrame) -> pd.DataFrame:
    # (Function remains the same)
    logging.info("Identifying missing/placeholder values...")
    df['FLAG_empty_body'] = df[BODY_COL].astype(str).str.strip() == ""; df['FLAG_empty_subject'] = df[SUBJECT_COL].astype(str).str.strip() == ""
    if MISSING_PLACEHOLDER: df['FLAG_placeholder_body'] = df[BODY_COL]==MISSING_PLACEHOLDER; df['FLAG_placeholder_subject'] = df[SUBJECT_COL]==MISSING_PLACEHOLDER
    logging.info("Missing/placeholder identification complete.")
    return df

def generate_summary(df: pd.DataFrame, split_name: str) -> Dict:
    # (Function remains the same)
    logging.info(f"Generating summary for {split_name}...")
    summary = {"split": split_name, "total_rows": len(df)}; flag_cols = [col for col in df.columns if col.startswith('FLAG_')]
    for flag_col in flag_cols:
        try: count=df[flag_col].sum(); percentage=(count/len(df)*100) if len(df)>0 else 0; summary[f"{flag_col}_count"]=int(count); summary[f"{flag_col}_percent"]=round(percentage,2)
        except Exception as e: logging.error(f"Error summarizing {flag_col}: {e}"); summary[f"{flag_col}_count"]="Error"; summary[f"{flag_col}_percent"]="Error"
    for col in ['body_word_count','subject_word_count','subject_body_rouge1','subject_body_rougeL']:
         if col in df.columns:
              try: summary[f'{col}_avg'] = round(df[col].mean(), 3)
              except Exception: summary[f'{col}_avg'] = np.nan
    logging.info(f"Summary for {split_name} generated.")
    return summary

# --- Main Execution ---
if __name__ == "__main__":
    original_stdout = sys.stdout; original_stderr = sys.stderr; logger = None; run_timestamp = get_timestamp()
    try:
        print("\n--- Starting Data Quality Analysis Script (7_v3) ---")
        # Get Paths
        cleaned_csv_files = select_files("Select ALL Cleaned CSV Files (Train, Dev, Test)", [("Cleaned CSV", "*_cleaned.csv"), ("CSV", "*.csv")])
        if not cleaned_csv_files: sys.exit("No files selected.")
        output_dir = select_folder("Select OUTPUT directory")
        if not output_dir: sys.exit("No output directory selected.")
        # Setup Logging
        os.makedirs(output_dir, exist_ok=True); log_filename=f"data_quality_analysis_log_{run_timestamp}.txt"; log_file_path=os.path.join(output_dir, log_filename)
        log_handlers = [logging.FileHandler(log_file_path), logging.StreamHandler(sys.stdout)]; [logging.root.removeHandler(h) for h in logging.root.handlers[:]]
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=log_handlers)
        logging.info(f"--- Script Start: {run_timestamp} ---"); logging.info(f"Input Files: {', '.join(map(os.path.basename, cleaned_csv_files))}"); logging.info(f"Output Dir: {output_dir}"); logging.info(f"Log File: {log_file_path}"); logging.info("-" * 30)

        all_summaries = []
        # Process Files
        for csv_file_path in cleaned_csv_files:
            split_name = os.path.basename(csv_file_path).split('_cleaned')[0]; logging.info(f"\n===== Processing: {split_name} =====")
            df = load_cleaned_data(csv_file_path)
            if df is None: logging.error(f"Skipping {csv_file_path}."); continue
            # Analyses
            df = calculate_lengths(df); df = flag_length_outliers(df); df = identify_missing_placeholders(df); df = identify_generic_subjects(df)
            df = calculate_subject_body_rouge(df) # CORRECTED VERSION v3
            # Summary
            summary = generate_summary(df, split_name); all_summaries.append(summary)
            # Save Annotated
            annotated_filename=f"{split_name}_quality_analyzed_{run_timestamp}.csv"; annotated_path=os.path.join(output_dir, annotated_filename)
            try: df.to_csv(annotated_path, index=False, encoding='utf-8'); logging.info(f"Saved annotated: {annotated_path}")
            except Exception as e: logging.error(f"Failed save: {annotated_path}: {e}", exc_info=True)
        # Save Overall Summary
        logging.info("\n===== Overall Summary =====")
        if all_summaries:
            df_summary=pd.DataFrame(all_summaries).set_index('split'); logging.info("Summary Report:\n"+df_summary.transpose().to_string())
            summary_filename=f"data_quality_summary_{run_timestamp}.csv"; summary_path=os.path.join(output_dir, summary_filename)
            try: df_summary.transpose().to_csv(summary_path, encoding='utf-8'); logging.info(f"Saved summary: {summary_path}"); print(f"\nSUCCESS: Results saved to {output_dir}")
            except Exception as e: logging.error(f"Failed save summary: {e}", exc_info=True)
        else: logging.warning("No summaries generated."); print("\nWARNING: No summaries generated.")
        logging.info(f"\n{'='*15} Script 7_v3 Finished {'='*15}")
    except SystemExit as e: logging.warning(f"Script exited: {e}")
    except FileNotFoundError as e: logging.error(f"ERROR: File not found: {e}"); print(f"ERROR: {e}", file=original_stderr)
    except ValueError as e: logging.error(f"ERROR: Data validation: {e}"); print(f"ERROR: {e}", file=original_stderr)
    except Exception as main_e: logging.exception(f"!!! UNEXPECTED ERROR: {main_e} !!!"); print(f"!!! UNEXPECTED ERROR: {main_e} !!!", file=original_stderr)
    finally: logging.shutdown(); sys.stdout = original_stdout; sys.stderr = original_stderr; print("\n(Logging finished)")